2024-02-12 02:07:33.164110: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-12 02:07:33.250830: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-02-12 02:07:33.250920: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-02-12 02:07:33.273185: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-02-12 02:07:33.322199: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-12 02:07:33.932238: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
1
0.09876694988459139
(1000, 16, 32, 4) (1000, 16, 32, 4)
1
(999, 16, 32, 4) (999, 16, 32, 4)
1
0.0986347007567667
(1000, 16, 32, 4) (1000, 16, 32, 4)
1
(999, 16, 32, 4) (999, 16, 32, 4)
0.020829969236160626
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 16, 32, 4)]          0         []                            
                                                                                                  
 rescaling (Rescaling)       (None, 16, 32, 4)            0         ['input_1[0][0]']             
                                                                                                  
 multi_head_attention (Mult  (None, 16, 32, 4)            308       ['rescaling[0][0]',           
 iHeadAttention)                                                     'rescaling[0][0]']           
                                                                                                  
 add (Add)                   (None, 16, 32, 4)            0         ['rescaling[0][0]',           
                                                                     'multi_head_attention[0][0]']
                                                                                                  
 layer_normalization (Layer  (None, 16, 32, 4)            8         ['add[0][0]']                 
 Normalization)                                                                                   
                                                                                                  
 conv2d (Conv2D)             (None, 16, 32, 64)           2368      ['layer_normalization[0][0]'] 
                                                                                                  
 batch_normalization (Batch  (None, 16, 32, 64)           256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 multi_head_attention_1 (Mu  (None, 16, 32, 64)           4208      ['batch_normalization[0][0]', 
 ltiHeadAttention)                                                   'batch_normalization[0][0]'] 
                                                                                                  
 add_1 (Add)                 (None, 16, 32, 64)           0         ['batch_normalization[0][0]', 
                                                                     'multi_head_attention_1[0][0]
                                                                    ']                            
                                                                                                  
 layer_normalization_1 (Lay  (None, 16, 32, 64)           128       ['add_1[0][0]']               
 erNormalization)                                                                                 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 16, 32, 64)           36928     ['layer_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 batch_normalization_1 (Bat  (None, 16, 32, 64)           256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 multi_head_attention_2 (Mu  (None, 16, 32, 64)           4208      ['batch_normalization_1[0][0]'
 ltiHeadAttention)                                                  , 'batch_normalization_1[0][0]
                                                                    ']                            
                                                                                                  
 add_2 (Add)                 (None, 16, 32, 64)           0         ['batch_normalization_1[0][0]'
                                                                    , 'multi_head_attention_2[0][0
                                                                    ]']                           
                                                                                                  
 layer_normalization_2 (Lay  (None, 16, 32, 64)           128       ['add_2[0][0]']               
 erNormalization)                                                                                 
                                                                                                  
 conv2d_2 (Conv2D)           (None, 16, 32, 64)           36928     ['layer_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 batch_normalization_2 (Bat  (None, 16, 32, 64)           256       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 multi_head_attention_3 (Mu  (None, 16, 32, 64)           4208      ['batch_normalization_2[0][0]'
 ltiHeadAttention)                                                  , 'batch_normalization_2[0][0]
                                                                    ']                            
                                                                                                  
 add_3 (Add)                 (None, 16, 32, 64)           0         ['batch_normalization_2[0][0]'
                                                                    , 'multi_head_attention_3[0][0
                                                                    ]']                           
                                                                                                  
 layer_normalization_3 (Lay  (None, 16, 32, 64)           128       ['add_3[0][0]']               
 erNormalization)                                                                                 
                                                                                                  
 conv2d_3 (Conv2D)           (None, 16, 32, 64)           36928     ['layer_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 batch_normalization_3 (Bat  (None, 16, 32, 64)           256       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 multi_head_attention_4 (Mu  (None, 16, 32, 64)           4208      ['batch_normalization_3[0][0]'
 ltiHeadAttention)                                                  , 'batch_normalization_3[0][0]
                                                                    ']                            
                                                                                                  
 add_4 (Add)                 (None, 16, 32, 64)           0         ['batch_normalization_3[0][0]'
                                                                    , 'multi_head_attention_4[0][0
                                                                    ]']                           
                                                                                                  
 layer_normalization_4 (Lay  (None, 16, 32, 64)           128       ['add_4[0][0]']               
 erNormalization)                                                                                 
                                                                                                  
 conv2d_4 (Conv2D)           (None, 16, 32, 64)           36928     ['layer_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 batch_normalization_4 (Bat  (None, 16, 32, 64)           256       ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 multi_head_attention_5 (Mu  (None, 16, 32, 64)           4208      ['batch_normalization_4[0][0]'
 ltiHeadAttention)                                                  , 'batch_normalization_4[0][0]
                                                                    ']                            
                                                                                                  
 add_5 (Add)                 (None, 16, 32, 64)           0         ['batch_normalization_4[0][0]'
                                                                    , 'multi_head_attention_5[0][0
                                                                    ]']                           
                                                                                                  
 layer_normalization_5 (Lay  (None, 16, 32, 64)           128       ['add_5[0][0]']               
 erNormalization)                                                                                 
                                                                                                  
 conv2d_5 (Conv2D)           (None, 16, 32, 64)           36928     ['layer_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 batch_normalization_5 (Bat  (None, 16, 32, 64)           256       ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 multi_head_attention_6 (Mu  (None, 16, 32, 64)           4208      ['batch_normalization_5[0][0]'
 ltiHeadAttention)                                                  , 'batch_normalization_5[0][0]
                                                                    ']                            
                                                                                                  
 add_6 (Add)                 (None, 16, 32, 64)           0         ['batch_normalization_5[0][0]'
                                                                    , 'multi_head_attention_6[0][0
                                                                    ]']                           
                                                                                                  
 layer_normalization_6 (Lay  (None, 16, 32, 64)           128       ['add_6[0][0]']               
 erNormalization)                                                                                 
                                                                                                  
 conv2d_6 (Conv2D)           (None, 16, 32, 64)           36928     ['layer_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 batch_normalization_6 (Bat  (None, 16, 32, 64)           256       ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 multi_head_attention_7 (Mu  (None, 16, 32, 64)           4208      ['batch_normalization_6[0][0]'
 ltiHeadAttention)                                                  , 'batch_normalization_6[0][0]
                                                                    ']                            
                                                                                                  
 add_7 (Add)                 (None, 16, 32, 64)           0         ['batch_normalization_6[0][0]'
                                                                    , 'multi_head_attention_7[0][0
                                                                    ]']                           
                                                                                                  
 layer_normalization_7 (Lay  (None, 16, 32, 64)           128       ['add_7[0][0]']               
 erNormalization)                                                                                 
                                                                                                  
 conv2d_7 (Conv2D)           (None, 16, 32, 64)           36928     ['layer_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 batch_normalization_7 (Bat  (None, 16, 32, 64)           256       ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 32, 4)            2308      ['batch_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
==================================================================================================
Total params: 295888 (1.13 MB)
Trainable params: 294864 (1.12 MB)
Non-trainable params: 1024 (4.00 KB)
__________________________________________________________________________________________________
Epoch 1/200

Epoch 1: val_loss improved from inf to 0.21112, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
/home/tim/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(
8/8 - 186s - loss: 0.4282 - val_loss: 0.2111 - 186s/epoch - 23s/step
Epoch 2/200

Epoch 2: val_loss improved from 0.21112 to 0.17052, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
8/8 - 195s - loss: 0.2751 - val_loss: 0.1705 - 195s/epoch - 24s/step
Epoch 3/200

Epoch 3: val_loss improved from 0.17052 to 0.13397, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
8/8 - 197s - loss: 0.1641 - val_loss: 0.1340 - 197s/epoch - 25s/step
Epoch 4/200

Epoch 4: val_loss improved from 0.13397 to 0.10647, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
8/8 - 190s - loss: 0.0626 - val_loss: 0.1065 - 190s/epoch - 24s/step
Epoch 5/200

Epoch 5: val_loss improved from 0.10647 to 0.06429, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
8/8 - 183s - loss: 0.0317 - val_loss: 0.0643 - 183s/epoch - 23s/step
Epoch 6/200

Epoch 6: val_loss improved from 0.06429 to 0.04899, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
8/8 - 191s - loss: 0.0242 - val_loss: 0.0490 - 191s/epoch - 24s/step
Epoch 7/200

Epoch 7: val_loss improved from 0.04899 to 0.03415, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
8/8 - 203s - loss: 0.0215 - val_loss: 0.0341 - 203s/epoch - 25s/step
Epoch 8/200

Epoch 8: val_loss improved from 0.03415 to 0.03056, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
8/8 - 200s - loss: 0.0202 - val_loss: 0.0306 - 200s/epoch - 25s/step
Epoch 9/200

Epoch 9: val_loss improved from 0.03056 to 0.02802, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
8/8 - 198s - loss: 0.0198 - val_loss: 0.0280 - 198s/epoch - 25s/step
Epoch 10/200

Epoch 10: val_loss improved from 0.02802 to 0.02342, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
8/8 - 197s - loss: 0.0199 - val_loss: 0.0234 - 197s/epoch - 25s/step
Epoch 11/200

Epoch 11: val_loss improved from 0.02342 to 0.02165, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
8/8 - 202s - loss: 0.0192 - val_loss: 0.0217 - 202s/epoch - 25s/step
Epoch 12/200

Epoch 12: val_loss improved from 0.02165 to 0.02133, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
8/8 - 194s - loss: 0.0190 - val_loss: 0.0213 - 194s/epoch - 24s/step
Epoch 13/200

Epoch 13: val_loss did not improve from 0.02133
8/8 - 193s - loss: 0.0187 - val_loss: 0.0229 - 193s/epoch - 24s/step
Epoch 14/200

Epoch 14: val_loss improved from 0.02133 to 0.02002, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
8/8 - 192s - loss: 0.0188 - val_loss: 0.0200 - 192s/epoch - 24s/step
Epoch 15/200

Epoch 15: val_loss did not improve from 0.02002
8/8 - 194s - loss: 0.0192 - val_loss: 0.0224 - 194s/epoch - 24s/step
Epoch 16/200

Epoch 16: val_loss did not improve from 0.02002
8/8 - 196s - loss: 0.0186 - val_loss: 0.0215 - 196s/epoch - 24s/step
Epoch 17/200

Epoch 17: val_loss did not improve from 0.02002
8/8 - 191s - loss: 0.0182 - val_loss: 0.0205 - 191s/epoch - 24s/step
Epoch 18/200

Epoch 18: val_loss improved from 0.02002 to 0.01844, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
8/8 - 196s - loss: 0.0182 - val_loss: 0.0184 - 196s/epoch - 24s/step
Epoch 19/200

Epoch 19: val_loss did not improve from 0.01844
8/8 - 198s - loss: 0.0178 - val_loss: 0.0193 - 198s/epoch - 25s/step
Epoch 20/200

Epoch 20: val_loss did not improve from 0.01844
8/8 - 197s - loss: 0.0178 - val_loss: 0.0197 - 197s/epoch - 25s/step
Epoch 21/200

Epoch 21: val_loss did not improve from 0.01844
8/8 - 193s - loss: 0.0180 - val_loss: 0.0207 - 193s/epoch - 24s/step
Epoch 22/200

Epoch 22: val_loss improved from 0.01844 to 0.01722, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
8/8 - 198s - loss: 0.0178 - val_loss: 0.0172 - 198s/epoch - 25s/step
Epoch 23/200

Epoch 23: val_loss did not improve from 0.01722
8/8 - 197s - loss: 0.0172 - val_loss: 0.0186 - 197s/epoch - 25s/step
Epoch 24/200

Epoch 24: val_loss did not improve from 0.01722
8/8 - 195s - loss: 0.0174 - val_loss: 0.0173 - 195s/epoch - 24s/step
Epoch 25/200

Epoch 25: val_loss improved from 0.01722 to 0.01672, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
8/8 - 195s - loss: 0.0174 - val_loss: 0.0167 - 195s/epoch - 24s/step
Epoch 26/200

Epoch 26: val_loss did not improve from 0.01672
8/8 - 194s - loss: 0.0174 - val_loss: 0.0176 - 194s/epoch - 24s/step
Epoch 27/200

Epoch 27: val_loss did not improve from 0.01672
8/8 - 196s - loss: 0.0171 - val_loss: 0.0171 - 196s/epoch - 24s/step
Epoch 28/200

Epoch 28: val_loss did not improve from 0.01672
8/8 - 197s - loss: 0.0168 - val_loss: 0.0169 - 197s/epoch - 25s/step
Epoch 29/200

Epoch 29: val_loss did not improve from 0.01672
8/8 - 198s - loss: 0.0168 - val_loss: 0.0169 - 198s/epoch - 25s/step
Epoch 30/200

Epoch 30: val_loss improved from 0.01672 to 0.01661, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
8/8 - 191s - loss: 0.0168 - val_loss: 0.0166 - 191s/epoch - 24s/step
Epoch 31/200

Epoch 31: val_loss did not improve from 0.01661
8/8 - 193s - loss: 0.0168 - val_loss: 0.0171 - 193s/epoch - 24s/step
Epoch 32/200

Epoch 32: val_loss did not improve from 0.01661
8/8 - 196s - loss: 0.0166 - val_loss: 0.0170 - 196s/epoch - 24s/step
Epoch 33/200

Epoch 33: val_loss improved from 0.01661 to 0.01646, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
8/8 - 198s - loss: 0.0167 - val_loss: 0.0165 - 198s/epoch - 25s/step
Epoch 34/200

Epoch 34: val_loss did not improve from 0.01646
8/8 - 195s - loss: 0.0164 - val_loss: 0.0166 - 195s/epoch - 24s/step
Epoch 35/200

Epoch 35: val_loss improved from 0.01646 to 0.01636, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
8/8 - 194s - loss: 0.0162 - val_loss: 0.0164 - 194s/epoch - 24s/step
Epoch 36/200

Epoch 36: val_loss improved from 0.01636 to 0.01616, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
8/8 - 196s - loss: 0.0162 - val_loss: 0.0162 - 196s/epoch - 24s/step
Epoch 37/200

Epoch 37: val_loss improved from 0.01616 to 0.01588, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
8/8 - 196s - loss: 0.0164 - val_loss: 0.0159 - 196s/epoch - 25s/step
Epoch 38/200

Epoch 38: val_loss did not improve from 0.01588
8/8 - 195s - loss: 0.0160 - val_loss: 0.0164 - 195s/epoch - 24s/step
Epoch 39/200

Epoch 39: val_loss did not improve from 0.01588
8/8 - 194s - loss: 0.0161 - val_loss: 0.0161 - 194s/epoch - 24s/step
Epoch 40/200

Epoch 40: val_loss improved from 0.01588 to 0.01576, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
8/8 - 193s - loss: 0.0160 - val_loss: 0.0158 - 193s/epoch - 24s/step
Epoch 41/200

Epoch 41: val_loss did not improve from 0.01576
8/8 - 195s - loss: 0.0158 - val_loss: 0.0158 - 195s/epoch - 24s/step
Epoch 42/200

Epoch 42: val_loss did not improve from 0.01576
8/8 - 196s - loss: 0.0161 - val_loss: 0.0158 - 196s/epoch - 24s/step
Epoch 43/200

Epoch 43: val_loss improved from 0.01576 to 0.01562, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
8/8 - 201s - loss: 0.0160 - val_loss: 0.0156 - 201s/epoch - 25s/step
Epoch 44/200

Epoch 44: val_loss did not improve from 0.01562
8/8 - 198s - loss: 0.0158 - val_loss: 0.0159 - 198s/epoch - 25s/step
Epoch 45/200

Epoch 45: val_loss did not improve from 0.01562
8/8 - 188s - loss: 0.0158 - val_loss: 0.0158 - 188s/epoch - 24s/step
Epoch 46/200

Epoch 46: val_loss did not improve from 0.01562
8/8 - 198s - loss: 0.0156 - val_loss: 0.0157 - 198s/epoch - 25s/step
Epoch 47/200

Epoch 47: val_loss did not improve from 0.01562
8/8 - 191s - loss: 0.0155 - val_loss: 0.0157 - 191s/epoch - 24s/step
Epoch 48/200

Epoch 48: val_loss improved from 0.01562 to 0.01543, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
8/8 - 194s - loss: 0.0153 - val_loss: 0.0154 - 194s/epoch - 24s/step
Epoch 49/200

Epoch 49: val_loss did not improve from 0.01543
8/8 - 196s - loss: 0.0155 - val_loss: 0.0162 - 196s/epoch - 24s/step
Epoch 50/200

Epoch 50: val_loss did not improve from 0.01543
8/8 - 193s - loss: 0.0156 - val_loss: 0.0166 - 193s/epoch - 24s/step
Epoch 51/200

Epoch 51: val_loss did not improve from 0.01543
8/8 - 195s - loss: 0.0158 - val_loss: 0.0161 - 195s/epoch - 24s/step
Epoch 52/200

Epoch 52: val_loss did not improve from 0.01543
8/8 - 194s - loss: 0.0151 - val_loss: 0.0158 - 194s/epoch - 24s/step
Epoch 53/200

Epoch 53: val_loss did not improve from 0.01543
8/8 - 192s - loss: 0.0151 - val_loss: 0.0157 - 192s/epoch - 24s/step
Epoch 54/200

Epoch 54: val_loss did not improve from 0.01543
8/8 - 200s - loss: 0.0150 - val_loss: 0.0160 - 200s/epoch - 25s/step
Epoch 55/200

Epoch 55: val_loss improved from 0.01543 to 0.01531, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
8/8 - 197s - loss: 0.0146 - val_loss: 0.0153 - 197s/epoch - 25s/step
Epoch 56/200

Epoch 56: val_loss did not improve from 0.01531
8/8 - 193s - loss: 0.0145 - val_loss: 0.0155 - 193s/epoch - 24s/step
Epoch 57/200

Epoch 57: val_loss did not improve from 0.01531
8/8 - 193s - loss: 0.0143 - val_loss: 0.0155 - 193s/epoch - 24s/step
Epoch 58/200

Epoch 58: val_loss did not improve from 0.01531
8/8 - 197s - loss: 0.0141 - val_loss: 0.0156 - 197s/epoch - 25s/step
Epoch 59/200

Epoch 59: val_loss did not improve from 0.01531
8/8 - 192s - loss: 0.0144 - val_loss: 0.0158 - 192s/epoch - 24s/step
Epoch 60/200

Epoch 60: val_loss did not improve from 0.01531
8/8 - 196s - loss: 0.0142 - val_loss: 0.0157 - 196s/epoch - 25s/step
Epoch 61/200

Epoch 61: val_loss did not improve from 0.01531
8/8 - 197s - loss: 0.0146 - val_loss: 0.0166 - 197s/epoch - 25s/step
Epoch 62/200

Epoch 62: val_loss did not improve from 0.01531
8/8 - 191s - loss: 0.0140 - val_loss: 0.0158 - 191s/epoch - 24s/step
Epoch 63/200

Epoch 63: val_loss did not improve from 0.01531
8/8 - 191s - loss: 0.0137 - val_loss: 0.0164 - 191s/epoch - 24s/step
Epoch 64/200

Epoch 64: val_loss did not improve from 0.01531
8/8 - 202s - loss: 0.0135 - val_loss: 0.0159 - 202s/epoch - 25s/step
Epoch 65/200

Epoch 65: val_loss did not improve from 0.01531
8/8 - 197s - loss: 0.0134 - val_loss: 0.0169 - 197s/epoch - 25s/step
Epoch 66/200

Epoch 66: val_loss did not improve from 0.01531
8/8 - 197s - loss: 0.0132 - val_loss: 0.0171 - 197s/epoch - 25s/step
Epoch 67/200

Epoch 67: val_loss did not improve from 0.01531
8/8 - 199s - loss: 0.0130 - val_loss: 0.0166 - 199s/epoch - 25s/step
Epoch 68/200

Epoch 68: val_loss did not improve from 0.01531
8/8 - 194s - loss: 0.0130 - val_loss: 0.0156 - 194s/epoch - 24s/step
Epoch 69/200

Epoch 69: val_loss improved from 0.01531 to 0.01529, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
8/8 - 200s - loss: 0.0130 - val_loss: 0.0153 - 200s/epoch - 25s/step
Epoch 70/200

Epoch 70: val_loss did not improve from 0.01529
8/8 - 202s - loss: 0.0127 - val_loss: 0.0154 - 202s/epoch - 25s/step
Epoch 71/200

Epoch 71: val_loss did not improve from 0.01529
8/8 - 200s - loss: 0.0127 - val_loss: 0.0170 - 200s/epoch - 25s/step
Epoch 72/200

Epoch 72: val_loss did not improve from 0.01529
8/8 - 198s - loss: 0.0131 - val_loss: 0.0154 - 198s/epoch - 25s/step
Epoch 73/200

Epoch 73: val_loss did not improve from 0.01529
8/8 - 201s - loss: 0.0129 - val_loss: 0.0154 - 201s/epoch - 25s/step
Epoch 74/200

Epoch 74: val_loss did not improve from 0.01529
8/8 - 191s - loss: 0.0125 - val_loss: 0.0157 - 191s/epoch - 24s/step
Epoch 75/200

Epoch 75: val_loss improved from 0.01529 to 0.01519, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
8/8 - 195s - loss: 0.0122 - val_loss: 0.0152 - 195s/epoch - 24s/step
Epoch 76/200

Epoch 76: val_loss did not improve from 0.01519
8/8 - 195s - loss: 0.0123 - val_loss: 0.0154 - 195s/epoch - 24s/step
Epoch 77/200

Epoch 77: val_loss did not improve from 0.01519
8/8 - 194s - loss: 0.0120 - val_loss: 0.0156 - 194s/epoch - 24s/step
Epoch 78/200

Epoch 78: val_loss did not improve from 0.01519
8/8 - 196s - loss: 0.0120 - val_loss: 0.0160 - 196s/epoch - 25s/step
Epoch 79/200

Epoch 79: val_loss did not improve from 0.01519
8/8 - 195s - loss: 0.0121 - val_loss: 0.0158 - 195s/epoch - 24s/step
Epoch 80/200

Epoch 80: val_loss did not improve from 0.01519
8/8 - 196s - loss: 0.0119 - val_loss: 0.0156 - 196s/epoch - 25s/step
Epoch 81/200

Epoch 81: val_loss did not improve from 0.01519
8/8 - 196s - loss: 0.0118 - val_loss: 0.0154 - 196s/epoch - 25s/step
Epoch 82/200

Epoch 82: val_loss did not improve from 0.01519
8/8 - 197s - loss: 0.0127 - val_loss: 0.0162 - 197s/epoch - 25s/step
Epoch 83/200

Epoch 83: val_loss did not improve from 0.01519
8/8 - 195s - loss: 0.0125 - val_loss: 0.0167 - 195s/epoch - 24s/step
Epoch 84/200

Epoch 84: val_loss did not improve from 0.01519
8/8 - 195s - loss: 0.0121 - val_loss: 0.0159 - 195s/epoch - 24s/step
Epoch 85/200

Epoch 85: val_loss did not improve from 0.01519
8/8 - 196s - loss: 0.0118 - val_loss: 0.0171 - 196s/epoch - 24s/step
Epoch 86/200

Epoch 86: val_loss did not improve from 0.01519
8/8 - 197s - loss: 0.0118 - val_loss: 0.0158 - 197s/epoch - 25s/step
Epoch 87/200

Epoch 87: val_loss did not improve from 0.01519
8/8 - 197s - loss: 0.0117 - val_loss: 0.0160 - 197s/epoch - 25s/step
Epoch 88/200

Epoch 88: val_loss did not improve from 0.01519
8/8 - 198s - loss: 0.0116 - val_loss: 0.0153 - 198s/epoch - 25s/step
Epoch 89/200

Epoch 89: val_loss did not improve from 0.01519
8/8 - 194s - loss: 0.0118 - val_loss: 0.0155 - 194s/epoch - 24s/step
Epoch 90/200

Epoch 90: val_loss did not improve from 0.01519
8/8 - 198s - loss: 0.0116 - val_loss: 0.0161 - 198s/epoch - 25s/step
Epoch 91/200

Epoch 91: val_loss did not improve from 0.01519
8/8 - 198s - loss: 0.0118 - val_loss: 0.0174 - 198s/epoch - 25s/step
Epoch 92/200

Epoch 92: val_loss did not improve from 0.01519
8/8 - 198s - loss: 0.0115 - val_loss: 0.0167 - 198s/epoch - 25s/step
Epoch 93/200

Epoch 93: val_loss did not improve from 0.01519
8/8 - 190s - loss: 0.0117 - val_loss: 0.0165 - 190s/epoch - 24s/step
Epoch 94/200

Epoch 94: val_loss did not improve from 0.01519
8/8 - 198s - loss: 0.0119 - val_loss: 0.0189 - 198s/epoch - 25s/step
Epoch 95/200

Epoch 95: val_loss did not improve from 0.01519
8/8 - 198s - loss: 0.0119 - val_loss: 0.0159 - 198s/epoch - 25s/step
Epoch 96/200

Epoch 96: val_loss improved from 0.01519 to 0.01502, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
8/8 - 194s - loss: 0.0118 - val_loss: 0.0150 - 194s/epoch - 24s/step
Epoch 97/200

Epoch 97: val_loss improved from 0.01502 to 0.01483, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
8/8 - 201s - loss: 0.0116 - val_loss: 0.0148 - 201s/epoch - 25s/step
Epoch 98/200

Epoch 98: val_loss improved from 0.01483 to 0.01468, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
8/8 - 196s - loss: 0.0112 - val_loss: 0.0147 - 196s/epoch - 25s/step
Epoch 99/200

Epoch 99: val_loss did not improve from 0.01468
8/8 - 194s - loss: 0.0109 - val_loss: 0.0148 - 194s/epoch - 24s/step
Epoch 100/200

Epoch 100: val_loss did not improve from 0.01468
8/8 - 195s - loss: 0.0119 - val_loss: 0.0158 - 195s/epoch - 24s/step
Epoch 101/200

Epoch 101: val_loss did not improve from 0.01468
8/8 - 197s - loss: 0.0116 - val_loss: 0.0153 - 197s/epoch - 25s/step
Epoch 102/200

Epoch 102: val_loss did not improve from 0.01468
8/8 - 195s - loss: 0.0110 - val_loss: 0.0149 - 195s/epoch - 24s/step
Epoch 103/200

Epoch 103: val_loss improved from 0.01468 to 0.01427, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
8/8 - 197s - loss: 0.0108 - val_loss: 0.0143 - 197s/epoch - 25s/step
Epoch 104/200

Epoch 104: val_loss did not improve from 0.01427
8/8 - 191s - loss: 0.0107 - val_loss: 0.0148 - 191s/epoch - 24s/step
Epoch 105/200

Epoch 105: val_loss did not improve from 0.01427
8/8 - 197s - loss: 0.0108 - val_loss: 0.0150 - 197s/epoch - 25s/step
Epoch 106/200

Epoch 106: val_loss did not improve from 0.01427
8/8 - 194s - loss: 0.0106 - val_loss: 0.0148 - 194s/epoch - 24s/step
Epoch 107/200

Epoch 107: val_loss did not improve from 0.01427
8/8 - 195s - loss: 0.0104 - val_loss: 0.0148 - 195s/epoch - 24s/step
Epoch 108/200

Epoch 108: val_loss did not improve from 0.01427
8/8 - 191s - loss: 0.0105 - val_loss: 0.0149 - 191s/epoch - 24s/step
Epoch 109/200

Epoch 109: val_loss did not improve from 0.01427
8/8 - 198s - loss: 0.0104 - val_loss: 0.0152 - 198s/epoch - 25s/step
Epoch 110/200

Epoch 110: val_loss did not improve from 0.01427
8/8 - 194s - loss: 0.0105 - val_loss: 0.0146 - 194s/epoch - 24s/step
Epoch 111/200

Epoch 111: val_loss did not improve from 0.01427
8/8 - 190s - loss: 0.0103 - val_loss: 0.0152 - 190s/epoch - 24s/step
Epoch 112/200

Epoch 112: val_loss did not improve from 0.01427
8/8 - 193s - loss: 0.0109 - val_loss: 0.0152 - 193s/epoch - 24s/step
Epoch 113/200

Epoch 113: val_loss did not improve from 0.01427
8/8 - 193s - loss: 0.0146 - val_loss: 0.0188 - 193s/epoch - 24s/step
Epoch 114/200

Epoch 114: val_loss did not improve from 0.01427
8/8 - 191s - loss: 0.0151 - val_loss: 0.0186 - 191s/epoch - 24s/step
Epoch 115/200

Epoch 115: val_loss did not improve from 0.01427
8/8 - 197s - loss: 0.0144 - val_loss: 0.0176 - 197s/epoch - 25s/step
Epoch 116/200

Epoch 116: val_loss did not improve from 0.01427
8/8 - 195s - loss: 0.0141 - val_loss: 0.0175 - 195s/epoch - 24s/step
Epoch 117/200

Epoch 117: val_loss did not improve from 0.01427
8/8 - 195s - loss: 0.0137 - val_loss: 0.0173 - 195s/epoch - 24s/step
Epoch 118/200

Epoch 118: val_loss did not improve from 0.01427
8/8 - 199s - loss: 0.0130 - val_loss: 0.0160 - 199s/epoch - 25s/step
Epoch 119/200

Epoch 119: val_loss did not improve from 0.01427
8/8 - 199s - loss: 0.0127 - val_loss: 0.0164 - 199s/epoch - 25s/step
Epoch 120/200

Epoch 120: val_loss did not improve from 0.01427
8/8 - 189s - loss: 0.0124 - val_loss: 0.0160 - 189s/epoch - 24s/step
Epoch 121/200

Epoch 121: val_loss did not improve from 0.01427
8/8 - 198s - loss: 0.0119 - val_loss: 0.0157 - 198s/epoch - 25s/step
Epoch 122/200

Epoch 122: val_loss did not improve from 0.01427
8/8 - 194s - loss: 0.0118 - val_loss: 0.0158 - 194s/epoch - 24s/step
Epoch 123/200

Epoch 123: val_loss did not improve from 0.01427
8/8 - 191s - loss: 0.0124 - val_loss: 0.0158 - 191s/epoch - 24s/step
Epoch 124/200

Epoch 124: val_loss did not improve from 0.01427
8/8 - 195s - loss: 0.0125 - val_loss: 0.0163 - 195s/epoch - 24s/step
Epoch 125/200

Epoch 125: val_loss did not improve from 0.01427
8/8 - 194s - loss: 0.0116 - val_loss: 0.0152 - 194s/epoch - 24s/step
Epoch 126/200

Epoch 126: val_loss did not improve from 0.01427
8/8 - 195s - loss: 0.0114 - val_loss: 0.0152 - 195s/epoch - 24s/step
Epoch 127/200

Epoch 127: val_loss did not improve from 0.01427
8/8 - 201s - loss: 0.0111 - val_loss: 0.0151 - 201s/epoch - 25s/step
Epoch 128/200

Epoch 128: val_loss did not improve from 0.01427
8/8 - 199s - loss: 0.0114 - val_loss: 0.0156 - 199s/epoch - 25s/step
Epoch 129/200

Epoch 129: val_loss did not improve from 0.01427
8/8 - 197s - loss: 0.0119 - val_loss: 0.0155 - 197s/epoch - 25s/step
Epoch 130/200

Epoch 130: val_loss did not improve from 0.01427
8/8 - 202s - loss: 0.0114 - val_loss: 0.0154 - 202s/epoch - 25s/step
Epoch 131/200

Epoch 131: val_loss did not improve from 0.01427
8/8 - 199s - loss: 0.0118 - val_loss: 0.0150 - 199s/epoch - 25s/step
Epoch 132/200

Epoch 132: val_loss did not improve from 0.01427
8/8 - 196s - loss: 0.0114 - val_loss: 0.0152 - 196s/epoch - 25s/step
Epoch 133/200

Epoch 133: val_loss did not improve from 0.01427
8/8 - 200s - loss: 0.0113 - val_loss: 0.0156 - 200s/epoch - 25s/step
Epoch 134/200

Epoch 134: val_loss did not improve from 0.01427
8/8 - 196s - loss: 0.0116 - val_loss: 0.0151 - 196s/epoch - 24s/step
Epoch 135/200

Epoch 135: val_loss did not improve from 0.01427
8/8 - 194s - loss: 0.0110 - val_loss: 0.0153 - 194s/epoch - 24s/step
Epoch 136/200

Epoch 136: val_loss did not improve from 0.01427
8/8 - 197s - loss: 0.0109 - val_loss: 0.0159 - 197s/epoch - 25s/step
Epoch 137/200

Epoch 137: val_loss did not improve from 0.01427
8/8 - 199s - loss: 0.0115 - val_loss: 0.0160 - 199s/epoch - 25s/step
Epoch 138/200

Epoch 138: val_loss did not improve from 0.01427
8/8 - 195s - loss: 0.0122 - val_loss: 0.0169 - 195s/epoch - 24s/step
Epoch 139/200

Epoch 139: val_loss did not improve from 0.01427
8/8 - 195s - loss: 0.0120 - val_loss: 0.0166 - 195s/epoch - 24s/step
Epoch 140/200

Epoch 140: val_loss did not improve from 0.01427
8/8 - 197s - loss: 0.0122 - val_loss: 0.0165 - 197s/epoch - 25s/step
Epoch 141/200

Epoch 141: val_loss did not improve from 0.01427
8/8 - 192s - loss: 0.0121 - val_loss: 0.0159 - 192s/epoch - 24s/step
Epoch 142/200

Epoch 142: val_loss did not improve from 0.01427
8/8 - 193s - loss: 0.0117 - val_loss: 0.0153 - 193s/epoch - 24s/step
Epoch 143/200

Epoch 143: val_loss did not improve from 0.01427
8/8 - 196s - loss: 0.0111 - val_loss: 0.0151 - 196s/epoch - 25s/step
Epoch 144/200

Epoch 144: val_loss did not improve from 0.01427
8/8 - 197s - loss: 0.0113 - val_loss: 0.0150 - 197s/epoch - 25s/step
Epoch 145/200

Epoch 145: val_loss did not improve from 0.01427
8/8 - 194s - loss: 0.0111 - val_loss: 0.0150 - 194s/epoch - 24s/step
Epoch 146/200

Epoch 146: val_loss did not improve from 0.01427
8/8 - 194s - loss: 0.0108 - val_loss: 0.0147 - 194s/epoch - 24s/step
Epoch 147/200

Epoch 147: val_loss did not improve from 0.01427
8/8 - 199s - loss: 0.0108 - val_loss: 0.0150 - 199s/epoch - 25s/step
Epoch 148/200

Epoch 148: val_loss did not improve from 0.01427
8/8 - 197s - loss: 0.0109 - val_loss: 0.0149 - 197s/epoch - 25s/step
Epoch 149/200

Epoch 149: val_loss did not improve from 0.01427
8/8 - 196s - loss: 0.0116 - val_loss: 0.0151 - 196s/epoch - 25s/step
Epoch 150/200

Epoch 150: val_loss did not improve from 0.01427
8/8 - 200s - loss: 0.0113 - val_loss: 0.0156 - 200s/epoch - 25s/step
Epoch 151/200

Epoch 151: val_loss did not improve from 0.01427
8/8 - 195s - loss: 0.0124 - val_loss: 0.0180 - 195s/epoch - 24s/step
Epoch 152/200

Epoch 152: val_loss did not improve from 0.01427
8/8 - 194s - loss: 0.0134 - val_loss: 0.0173 - 194s/epoch - 24s/step
Epoch 153/200

Epoch 153: val_loss did not improve from 0.01427
8/8 - 196s - loss: 0.0132 - val_loss: 0.0163 - 196s/epoch - 24s/step
Epoch 154/200

Epoch 154: val_loss did not improve from 0.01427
8/8 - 198s - loss: 0.0123 - val_loss: 0.0158 - 198s/epoch - 25s/step
Epoch 155/200

Epoch 155: val_loss did not improve from 0.01427
8/8 - 195s - loss: 0.0121 - val_loss: 0.0155 - 195s/epoch - 24s/step
Epoch 156/200

Epoch 156: val_loss did not improve from 0.01427
8/8 - 198s - loss: 0.0116 - val_loss: 0.0150 - 198s/epoch - 25s/step
Epoch 157/200

Epoch 157: val_loss did not improve from 0.01427
8/8 - 192s - loss: 0.0118 - val_loss: 0.0149 - 192s/epoch - 24s/step
Epoch 158/200

Epoch 158: val_loss did not improve from 0.01427
8/8 - 195s - loss: 0.0120 - val_loss: 0.0158 - 195s/epoch - 24s/step
Epoch 159/200

Epoch 159: val_loss did not improve from 0.01427
8/8 - 197s - loss: 0.0116 - val_loss: 0.0154 - 197s/epoch - 25s/step
Epoch 160/200

Epoch 160: val_loss did not improve from 0.01427
8/8 - 197s - loss: 0.0116 - val_loss: 0.0152 - 197s/epoch - 25s/step
Epoch 161/200

Epoch 161: val_loss did not improve from 0.01427
8/8 - 198s - loss: 0.0115 - val_loss: 0.0150 - 198s/epoch - 25s/step
Epoch 162/200

Epoch 162: val_loss did not improve from 0.01427
8/8 - 196s - loss: 0.0113 - val_loss: 0.0148 - 196s/epoch - 24s/step
Epoch 163/200

Epoch 163: val_loss did not improve from 0.01427
8/8 - 192s - loss: 0.0110 - val_loss: 0.0145 - 192s/epoch - 24s/step
Epoch 164/200

Epoch 164: val_loss did not improve from 0.01427
8/8 - 197s - loss: 0.0114 - val_loss: 0.0149 - 197s/epoch - 25s/step
Epoch 165/200

Epoch 165: val_loss did not improve from 0.01427
8/8 - 194s - loss: 0.0113 - val_loss: 0.0153 - 194s/epoch - 24s/step
Epoch 166/200

Epoch 166: val_loss did not improve from 0.01427
8/8 - 198s - loss: 0.0109 - val_loss: 0.0147 - 198s/epoch - 25s/step
Epoch 167/200

Epoch 167: val_loss did not improve from 0.01427
8/8 - 194s - loss: 0.0109 - val_loss: 0.0146 - 194s/epoch - 24s/step
Epoch 168/200

Epoch 168: val_loss did not improve from 0.01427
8/8 - 196s - loss: 0.0107 - val_loss: 0.0147 - 196s/epoch - 24s/step
Epoch 169/200

Epoch 169: val_loss did not improve from 0.01427
8/8 - 194s - loss: 0.0110 - val_loss: 0.0150 - 194s/epoch - 24s/step
Epoch 170/200

Epoch 170: val_loss did not improve from 0.01427
8/8 - 191s - loss: 0.0106 - val_loss: 0.0148 - 191s/epoch - 24s/step
Epoch 171/200

Epoch 171: val_loss did not improve from 0.01427
8/8 - 189s - loss: 0.0106 - val_loss: 0.0145 - 189s/epoch - 24s/step
Epoch 172/200

Epoch 172: val_loss did not improve from 0.01427
8/8 - 186s - loss: 0.0105 - val_loss: 0.0144 - 186s/epoch - 23s/step
Epoch 173/200

Epoch 173: val_loss did not improve from 0.01427
8/8 - 195s - loss: 0.0105 - val_loss: 0.0145 - 195s/epoch - 24s/step
Epoch 174/200

Epoch 174: val_loss did not improve from 0.01427
8/8 - 193s - loss: 0.0105 - val_loss: 0.0145 - 193s/epoch - 24s/step
Epoch 175/200

Epoch 175: val_loss improved from 0.01427 to 0.01421, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
8/8 - 172s - loss: 0.0105 - val_loss: 0.0142 - 172s/epoch - 21s/step
Epoch 176/200

Epoch 176: val_loss did not improve from 0.01421
8/8 - 167s - loss: 0.0104 - val_loss: 0.0144 - 167s/epoch - 21s/step
Epoch 177/200

Epoch 177: val_loss did not improve from 0.01421
8/8 - 151s - loss: 0.0106 - val_loss: 0.0145 - 151s/epoch - 19s/step
Epoch 178/200

Epoch 178: val_loss did not improve from 0.01421
8/8 - 165s - loss: 0.0106 - val_loss: 0.0145 - 165s/epoch - 21s/step
Epoch 179/200

Epoch 179: val_loss did not improve from 0.01421
8/8 - 170s - loss: 0.0105 - val_loss: 0.0146 - 170s/epoch - 21s/step
Epoch 180/200

Epoch 180: val_loss did not improve from 0.01421
8/8 - 187s - loss: 0.0105 - val_loss: 0.0149 - 187s/epoch - 23s/step
Epoch 181/200

Epoch 181: val_loss did not improve from 0.01421
8/8 - 192s - loss: 0.0105 - val_loss: 0.0144 - 192s/epoch - 24s/step
Epoch 182/200

Epoch 182: val_loss did not improve from 0.01421
8/8 - 173s - loss: 0.0103 - val_loss: 0.0149 - 173s/epoch - 22s/step
Epoch 183/200

Epoch 183: val_loss did not improve from 0.01421
8/8 - 178s - loss: 0.0105 - val_loss: 0.0150 - 178s/epoch - 22s/step
Epoch 184/200

Epoch 184: val_loss did not improve from 0.01421
8/8 - 187s - loss: 0.0103 - val_loss: 0.0147 - 187s/epoch - 23s/step
Epoch 185/200

Epoch 185: val_loss did not improve from 0.01421
8/8 - 180s - loss: 0.0101 - val_loss: 0.0143 - 180s/epoch - 23s/step
Epoch 186/200

Epoch 186: val_loss did not improve from 0.01421
8/8 - 191s - loss: 0.0101 - val_loss: 0.0149 - 191s/epoch - 24s/step
Epoch 187/200

Epoch 187: val_loss did not improve from 0.01421
8/8 - 183s - loss: 0.0102 - val_loss: 0.0145 - 183s/epoch - 23s/step
Epoch 188/200

Epoch 188: val_loss did not improve from 0.01421
8/8 - 158s - loss: 0.0110 - val_loss: 0.0201 - 158s/epoch - 20s/step
Epoch 189/200

Epoch 189: val_loss did not improve from 0.01421
8/8 - 187s - loss: 0.0110 - val_loss: 0.0153 - 187s/epoch - 23s/step
Epoch 190/200

Epoch 190: val_loss did not improve from 0.01421
8/8 - 195s - loss: 0.0107 - val_loss: 0.0148 - 195s/epoch - 24s/step
Epoch 191/200

Epoch 191: val_loss did not improve from 0.01421
8/8 - 197s - loss: 0.0108 - val_loss: 0.0164 - 197s/epoch - 25s/step
Epoch 192/200

Epoch 192: val_loss did not improve from 0.01421
8/8 - 169s - loss: 0.0121 - val_loss: 0.0152 - 169s/epoch - 21s/step
Epoch 193/200

Epoch 193: val_loss did not improve from 0.01421
8/8 - 172s - loss: 0.0115 - val_loss: 0.0154 - 172s/epoch - 22s/step
Epoch 194/200

Epoch 194: val_loss did not improve from 0.01421
8/8 - 191s - loss: 0.0112 - val_loss: 0.0154 - 191s/epoch - 24s/step
Epoch 195/200

Epoch 195: val_loss did not improve from 0.01421
8/8 - 187s - loss: 0.0108 - val_loss: 0.0152 - 187s/epoch - 23s/step
Epoch 196/200

Epoch 196: val_loss did not improve from 0.01421
8/8 - 191s - loss: 0.0107 - val_loss: 0.0154 - 191s/epoch - 24s/step
Epoch 197/200

Epoch 197: val_loss did not improve from 0.01421
8/8 - 190s - loss: 0.0107 - val_loss: 0.0147 - 190s/epoch - 24s/step
Epoch 198/200

Epoch 198: val_loss did not improve from 0.01421
8/8 - 185s - loss: 0.0105 - val_loss: 0.0147 - 185s/epoch - 23s/step
Epoch 199/200

Epoch 199: val_loss did not improve from 0.01421
8/8 - 191s - loss: 0.0105 - val_loss: 0.0152 - 191s/epoch - 24s/step
Epoch 200/200

Epoch 200: val_loss did not improve from 0.01421
8/8 - 191s - loss: 0.0105 - val_loss: 0.0145 - 191s/epoch - 24s/step
 1/32 [..............................] - ETA: 1:08 2/32 [>.............................] - ETA: 24s  3/32 [=>............................] - ETA: 25s 4/32 [==>...........................] - ETA: 23s 5/32 [===>..........................] - ETA: 22s 6/32 [====>.........................] - ETA: 22s 7/32 [=====>........................] - ETA: 21s 8/32 [======>.......................] - ETA: 20s 9/32 [=======>......................] - ETA: 19s10/32 [========>.....................] - ETA: 18s11/32 [=========>....................] - ETA: 17s12/32 [==========>...................] - ETA: 16s13/32 [===========>..................] - ETA: 16s14/32 [============>.................] - ETA: 15s15/32 [=============>................] - ETA: 14s16/32 [==============>...............] - ETA: 13s17/32 [==============>...............] - ETA: 12s18/32 [===============>..............] - ETA: 11s19/32 [================>.............] - ETA: 10s20/32 [=================>............] - ETA: 10s21/32 [==================>...........] - ETA: 9s 22/32 [===================>..........] - ETA: 8s23/32 [====================>.........] - ETA: 7s24/32 [=====================>........] - ETA: 6s25/32 [======================>.......] - ETA: 5s26/32 [=======================>......] - ETA: 5s27/32 [========================>.....] - ETA: 4s28/32 [=========================>....] - ETA: 3s29/32 [==========================>...] - ETA: 2s30/32 [===========================>..] - ETA: 1s31/32 [============================>.] - ETA: 0s32/32 [==============================] - ETA: 0s32/32 [==============================] - 28s 821ms/step
0.9278908904875056
