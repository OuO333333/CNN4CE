2024-02-13 18:15:20.289959: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-13 18:15:20.315489: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-02-13 18:15:20.315530: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-02-13 18:15:20.316307: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-02-13 18:15:20.320344: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-13 18:15:20.789576: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-02-13 18:15:21.600793: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-02-13 18:15:21.621670: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-02-13 18:15:21.621822: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-02-13 18:15:21.622425: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-02-13 18:15:21.622517: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-02-13 18:15:21.622593: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-02-13 18:15:21.675582: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-02-13 18:15:21.675704: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-02-13 18:15:21.675787: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-02-13 18:15:21.675865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5998 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6
epochs_num =  40
batch_size_num =  32
encoder_block_num =  9
learning_rate_num =  0.0001
SNR =  0.1
1
0.0987170177202544
(1000, 16, 32, 4) (1000, 16, 32, 4)
1
(999, 16, 32, 4) (999, 16, 32, 4)
1
0.09857882531099257
(1000, 16, 32, 4) (1000, 16, 32, 4)
1
(999, 16, 32, 4) (999, 16, 32, 4)
0.020829969236160626
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 16, 32, 4)]          0         []                            
                                                                                                  
 rescaling (Rescaling)       (None, 16, 32, 4)            0         ['input_1[0][0]']             
                                                                                                  
 multi_head_attention (Mult  (None, 16, 32, 4)            308       ['rescaling[0][0]',           
 iHeadAttention)                                                     'rescaling[0][0]']           
                                                                                                  
 add (Add)                   (None, 16, 32, 4)            0         ['rescaling[0][0]',           
                                                                     'multi_head_attention[0][0]']
                                                                                                  
 layer_normalization (Layer  (None, 16, 32, 4)            8         ['add[0][0]']                 
 Normalization)                                                                                   
                                                                                                  
 conv2d (Conv2D)             (None, 16, 32, 64)           2368      ['layer_normalization[0][0]'] 
                                                                                                  
 batch_normalization (Batch  (None, 16, 32, 64)           256       ['conv2d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 multi_head_attention_1 (Mu  (None, 16, 32, 64)           4208      ['batch_normalization[0][0]', 
 ltiHeadAttention)                                                   'batch_normalization[0][0]'] 
                                                                                                  
 add_1 (Add)                 (None, 16, 32, 64)           0         ['batch_normalization[0][0]', 
                                                                     'multi_head_attention_1[0][0]
                                                                    ']                            
                                                                                                  
 layer_normalization_1 (Lay  (None, 16, 32, 64)           128       ['add_1[0][0]']               
 erNormalization)                                                                                 
                                                                                                  
 conv2d_1 (Conv2D)           (None, 16, 32, 64)           36928     ['layer_normalization_1[0][0]'
                                                                    ]                             
                                                                                                  
 batch_normalization_1 (Bat  (None, 16, 32, 64)           256       ['conv2d_1[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 multi_head_attention_2 (Mu  (None, 16, 32, 64)           4208      ['batch_normalization_1[0][0]'
 ltiHeadAttention)                                                  , 'batch_normalization_1[0][0]
                                                                    ']                            
                                                                                                  
 add_2 (Add)                 (None, 16, 32, 64)           0         ['batch_normalization_1[0][0]'
                                                                    , 'multi_head_attention_2[0][0
                                                                    ]']                           
                                                                                                  
 layer_normalization_2 (Lay  (None, 16, 32, 64)           128       ['add_2[0][0]']               
 erNormalization)                                                                                 
                                                                                                  
 conv2d_2 (Conv2D)           (None, 16, 32, 64)           36928     ['layer_normalization_2[0][0]'
                                                                    ]                             
                                                                                                  
 batch_normalization_2 (Bat  (None, 16, 32, 64)           256       ['conv2d_2[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 multi_head_attention_3 (Mu  (None, 16, 32, 64)           4208      ['batch_normalization_2[0][0]'
 ltiHeadAttention)                                                  , 'batch_normalization_2[0][0]
                                                                    ']                            
                                                                                                  
 add_3 (Add)                 (None, 16, 32, 64)           0         ['batch_normalization_2[0][0]'
                                                                    , 'multi_head_attention_3[0][0
                                                                    ]']                           
                                                                                                  
 layer_normalization_3 (Lay  (None, 16, 32, 64)           128       ['add_3[0][0]']               
 erNormalization)                                                                                 
                                                                                                  
 conv2d_3 (Conv2D)           (None, 16, 32, 64)           36928     ['layer_normalization_3[0][0]'
                                                                    ]                             
                                                                                                  
 batch_normalization_3 (Bat  (None, 16, 32, 64)           256       ['conv2d_3[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 multi_head_attention_4 (Mu  (None, 16, 32, 64)           4208      ['batch_normalization_3[0][0]'
 ltiHeadAttention)                                                  , 'batch_normalization_3[0][0]
                                                                    ']                            
                                                                                                  
 add_4 (Add)                 (None, 16, 32, 64)           0         ['batch_normalization_3[0][0]'
                                                                    , 'multi_head_attention_4[0][0
                                                                    ]']                           
                                                                                                  
 layer_normalization_4 (Lay  (None, 16, 32, 64)           128       ['add_4[0][0]']               
 erNormalization)                                                                                 
                                                                                                  
 conv2d_4 (Conv2D)           (None, 16, 32, 64)           36928     ['layer_normalization_4[0][0]'
                                                                    ]                             
                                                                                                  
 batch_normalization_4 (Bat  (None, 16, 32, 64)           256       ['conv2d_4[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 multi_head_attention_5 (Mu  (None, 16, 32, 64)           4208      ['batch_normalization_4[0][0]'
 ltiHeadAttention)                                                  , 'batch_normalization_4[0][0]
                                                                    ']                            
                                                                                                  
 add_5 (Add)                 (None, 16, 32, 64)           0         ['batch_normalization_4[0][0]'
                                                                    , 'multi_head_attention_5[0][0
                                                                    ]']                           
                                                                                                  
 layer_normalization_5 (Lay  (None, 16, 32, 64)           128       ['add_5[0][0]']               
 erNormalization)                                                                                 
                                                                                                  
 conv2d_5 (Conv2D)           (None, 16, 32, 64)           36928     ['layer_normalization_5[0][0]'
                                                                    ]                             
                                                                                                  
 batch_normalization_5 (Bat  (None, 16, 32, 64)           256       ['conv2d_5[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 multi_head_attention_6 (Mu  (None, 16, 32, 64)           4208      ['batch_normalization_5[0][0]'
 ltiHeadAttention)                                                  , 'batch_normalization_5[0][0]
                                                                    ']                            
                                                                                                  
 add_6 (Add)                 (None, 16, 32, 64)           0         ['batch_normalization_5[0][0]'
                                                                    , 'multi_head_attention_6[0][0
                                                                    ]']                           
                                                                                                  
 layer_normalization_6 (Lay  (None, 16, 32, 64)           128       ['add_6[0][0]']               
 erNormalization)                                                                                 
                                                                                                  
 conv2d_6 (Conv2D)           (None, 16, 32, 64)           36928     ['layer_normalization_6[0][0]'
                                                                    ]                             
                                                                                                  
 batch_normalization_6 (Bat  (None, 16, 32, 64)           256       ['conv2d_6[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 multi_head_attention_7 (Mu  (None, 16, 32, 64)           4208      ['batch_normalization_6[0][0]'
 ltiHeadAttention)                                                  , 'batch_normalization_6[0][0]
                                                                    ']                            
                                                                                                  
 add_7 (Add)                 (None, 16, 32, 64)           0         ['batch_normalization_6[0][0]'
                                                                    , 'multi_head_attention_7[0][0
                                                                    ]']                           
                                                                                                  
 layer_normalization_7 (Lay  (None, 16, 32, 64)           128       ['add_7[0][0]']               
 erNormalization)                                                                                 
                                                                                                  
 conv2d_7 (Conv2D)           (None, 16, 32, 64)           36928     ['layer_normalization_7[0][0]'
                                                                    ]                             
                                                                                                  
 batch_normalization_7 (Bat  (None, 16, 32, 64)           256       ['conv2d_7[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 multi_head_attention_8 (Mu  (None, 16, 32, 64)           4208      ['batch_normalization_7[0][0]'
 ltiHeadAttention)                                                  , 'batch_normalization_7[0][0]
                                                                    ']                            
                                                                                                  
 add_8 (Add)                 (None, 16, 32, 64)           0         ['batch_normalization_7[0][0]'
                                                                    , 'multi_head_attention_8[0][0
                                                                    ]']                           
                                                                                                  
 layer_normalization_8 (Lay  (None, 16, 32, 64)           128       ['add_8[0][0]']               
 erNormalization)                                                                                 
                                                                                                  
 conv2d_8 (Conv2D)           (None, 16, 32, 64)           36928     ['layer_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
 batch_normalization_8 (Bat  (None, 16, 32, 64)           256       ['conv2d_8[0][0]']            
 chNormalization)                                                                                 
                                                                                                  
 conv2d_9 (Conv2D)           (None, 16, 32, 4)            2308      ['batch_normalization_8[0][0]'
                                                                    ]                             
                                                                                                  
==================================================================================================
Total params: 337408 (1.29 MB)
Trainable params: 336256 (1.28 MB)
Non-trainable params: 1152 (4.50 KB)
__________________________________________________________________________________________________
Epoch 1/40
2024-02-13 18:15:28.221138: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907
2024-02-13 18:15:28.777145: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fe8d83ae570 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-02-13 18:15:28.777181: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Ti, Compute Capability 8.6
2024-02-13 18:15:28.780636: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1707819328.831376   41424 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.

Epoch 1: val_loss improved from inf to 0.14830, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
/home/tim/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(
29/29 - 17s - loss: 0.2828 - val_loss: 0.1483 - 17s/epoch - 582ms/step
Epoch 2/40

Epoch 2: val_loss improved from 0.14830 to 0.07435, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
29/29 - 4s - loss: 0.0431 - val_loss: 0.0744 - 4s/epoch - 154ms/step
Epoch 3/40

Epoch 3: val_loss improved from 0.07435 to 0.04218, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
29/29 - 4s - loss: 0.0205 - val_loss: 0.0422 - 4s/epoch - 152ms/step
Epoch 4/40

Epoch 4: val_loss did not improve from 0.04218
29/29 - 4s - loss: 0.0185 - val_loss: 0.0754 - 4s/epoch - 148ms/step
Epoch 5/40

Epoch 5: val_loss improved from 0.04218 to 0.03056, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
29/29 - 4s - loss: 0.0194 - val_loss: 0.0306 - 4s/epoch - 152ms/step
Epoch 6/40

Epoch 6: val_loss improved from 0.03056 to 0.02673, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
29/29 - 4s - loss: 0.0170 - val_loss: 0.0267 - 4s/epoch - 152ms/step
Epoch 7/40

Epoch 7: val_loss improved from 0.02673 to 0.02099, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
29/29 - 4s - loss: 0.0165 - val_loss: 0.0210 - 4s/epoch - 152ms/step
Epoch 8/40

Epoch 8: val_loss improved from 0.02099 to 0.01610, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
29/29 - 4s - loss: 0.0164 - val_loss: 0.0161 - 4s/epoch - 152ms/step
Epoch 9/40

Epoch 9: val_loss did not improve from 0.01610
29/29 - 4s - loss: 0.0158 - val_loss: 0.0214 - 4s/epoch - 147ms/step
Epoch 10/40

Epoch 10: val_loss improved from 0.01610 to 0.01582, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
29/29 - 4s - loss: 0.0159 - val_loss: 0.0158 - 4s/epoch - 152ms/step
Epoch 11/40

Epoch 11: val_loss improved from 0.01582 to 0.01575, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
29/29 - 5s - loss: 0.0147 - val_loss: 0.0158 - 5s/epoch - 159ms/step
Epoch 12/40

Epoch 12: val_loss did not improve from 0.01575
29/29 - 5s - loss: 0.0146 - val_loss: 0.0164 - 5s/epoch - 155ms/step
Epoch 13/40

Epoch 13: val_loss improved from 0.01575 to 0.01542, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
29/29 - 4s - loss: 0.0154 - val_loss: 0.0154 - 4s/epoch - 153ms/step
Epoch 14/40

Epoch 14: val_loss did not improve from 0.01542
29/29 - 4s - loss: 0.0136 - val_loss: 0.0156 - 4s/epoch - 147ms/step
Epoch 15/40

Epoch 15: val_loss did not improve from 0.01542
29/29 - 4s - loss: 0.0129 - val_loss: 0.0158 - 4s/epoch - 147ms/step
Epoch 16/40

Epoch 16: val_loss did not improve from 0.01542
29/29 - 4s - loss: 0.0124 - val_loss: 0.0154 - 4s/epoch - 147ms/step
Epoch 17/40

Epoch 17: val_loss improved from 0.01542 to 0.01515, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
29/29 - 4s - loss: 0.0125 - val_loss: 0.0151 - 4s/epoch - 152ms/step
Epoch 18/40

Epoch 18: val_loss did not improve from 0.01515
29/29 - 4s - loss: 0.0123 - val_loss: 0.0157 - 4s/epoch - 147ms/step
Epoch 19/40

Epoch 19: val_loss did not improve from 0.01515
29/29 - 4s - loss: 0.0120 - val_loss: 0.0158 - 4s/epoch - 147ms/step
Epoch 20/40

Epoch 20: val_loss did not improve from 0.01515
29/29 - 4s - loss: 0.0116 - val_loss: 0.0192 - 4s/epoch - 147ms/step
Epoch 21/40

Epoch 21: val_loss did not improve from 0.01515
29/29 - 4s - loss: 0.0120 - val_loss: 0.0158 - 4s/epoch - 147ms/step
Epoch 22/40

Epoch 22: val_loss improved from 0.01515 to 0.01484, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
29/29 - 4s - loss: 0.0113 - val_loss: 0.0148 - 4s/epoch - 152ms/step
Epoch 23/40

Epoch 23: val_loss improved from 0.01484 to 0.01465, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
29/29 - 4s - loss: 0.0109 - val_loss: 0.0147 - 4s/epoch - 152ms/step
Epoch 24/40

Epoch 24: val_loss did not improve from 0.01465
29/29 - 4s - loss: 0.0106 - val_loss: 0.0151 - 4s/epoch - 147ms/step
Epoch 25/40

Epoch 25: val_loss improved from 0.01465 to 0.01452, saving model to CNN_UMi_3path_2fre_SNRminus10dB_200ep.hdf5
29/29 - 4s - loss: 0.0105 - val_loss: 0.0145 - 4s/epoch - 152ms/step
Epoch 26/40

Epoch 26: val_loss did not improve from 0.01452
29/29 - 4s - loss: 0.0105 - val_loss: 0.0159 - 4s/epoch - 147ms/step
Epoch 27/40

Epoch 27: val_loss did not improve from 0.01452
29/29 - 4s - loss: 0.0106 - val_loss: 0.0153 - 4s/epoch - 147ms/step
Epoch 28/40

Epoch 28: val_loss did not improve from 0.01452
29/29 - 4s - loss: 0.0109 - val_loss: 0.0151 - 4s/epoch - 149ms/step
Epoch 29/40

Epoch 29: val_loss did not improve from 0.01452
29/29 - 5s - loss: 0.0099 - val_loss: 0.0167 - 5s/epoch - 155ms/step
Epoch 30/40

Epoch 30: val_loss did not improve from 0.01452
29/29 - 4s - loss: 0.0095 - val_loss: 0.0149 - 4s/epoch - 152ms/step
Epoch 31/40

Epoch 31: val_loss did not improve from 0.01452
29/29 - 4s - loss: 0.0093 - val_loss: 0.0147 - 4s/epoch - 149ms/step
Epoch 32/40

Epoch 32: val_loss did not improve from 0.01452
29/29 - 4s - loss: 0.0093 - val_loss: 0.0148 - 4s/epoch - 147ms/step
Epoch 33/40

Epoch 33: val_loss did not improve from 0.01452
29/29 - 4s - loss: 0.0087 - val_loss: 0.0149 - 4s/epoch - 150ms/step
Epoch 34/40

Epoch 34: val_loss did not improve from 0.01452
29/29 - 4s - loss: 0.0088 - val_loss: 0.0148 - 4s/epoch - 148ms/step
Epoch 35/40

Epoch 35: val_loss did not improve from 0.01452
29/29 - 4s - loss: 0.0084 - val_loss: 0.0170 - 4s/epoch - 147ms/step
Epoch 36/40

Epoch 36: val_loss did not improve from 0.01452
29/29 - 4s - loss: 0.0083 - val_loss: 0.0153 - 4s/epoch - 147ms/step
Epoch 37/40

Epoch 37: val_loss did not improve from 0.01452
29/29 - 4s - loss: 0.0078 - val_loss: 0.0156 - 4s/epoch - 147ms/step
Epoch 38/40

Epoch 38: val_loss did not improve from 0.01452
29/29 - 4s - loss: 0.0076 - val_loss: 0.0155 - 4s/epoch - 147ms/step
Epoch 39/40

Epoch 39: val_loss did not improve from 0.01452
29/29 - 4s - loss: 0.0074 - val_loss: 0.0158 - 4s/epoch - 147ms/step
Epoch 40/40

Epoch 40: val_loss did not improve from 0.01452
29/29 - 4s - loss: 0.0076 - val_loss: 0.0157 - 4s/epoch - 147ms/step
 1/32 [..............................] - ETA: 10s 2/32 [>.............................] - ETA: 1s  3/32 [=>............................] - ETA: 1s 4/32 [==>...........................] - ETA: 1s 5/32 [===>..........................] - ETA: 1s 6/32 [====>.........................] - ETA: 1s 7/32 [=====>........................] - ETA: 1s 8/32 [======>.......................] - ETA: 1s 9/32 [=======>......................] - ETA: 1s10/32 [========>.....................] - ETA: 1s11/32 [=========>....................] - ETA: 1s12/32 [==========>...................] - ETA: 1s13/32 [===========>..................] - ETA: 0s14/32 [============>.................] - ETA: 0s15/32 [=============>................] - ETA: 0s16/32 [==============>...............] - ETA: 0s17/32 [==============>...............] - ETA: 0s18/32 [===============>..............] - ETA: 0s19/32 [================>.............] - ETA: 0s20/32 [=================>............] - ETA: 0s21/32 [==================>...........] - ETA: 0s22/32 [===================>..........] - ETA: 0s23/32 [====================>.........] - ETA: 0s24/32 [=====================>........] - ETA: 0s25/32 [======================>.......] - ETA: 0s26/32 [=======================>......] - ETA: 0s27/32 [========================>.....] - ETA: 0s28/32 [=========================>....] - ETA: 0s29/32 [==========================>...] - ETA: 0s30/32 [===========================>..] - ETA: 0s31/32 [============================>.] - ETA: 0s32/32 [==============================] - ETA: 0s32/32 [==============================] - 2s 54ms/step
0.8134316863427159
