command:
python3 SF_CNN_2fre_train.py > train1.log 2>&1
python3 SF_CNN_2fre_train_further.py > train2.log 2>&1
python3 SF_CNN_2fre_test.py > test.log 2>&1

GPU: NVIDIA GeForce RTX 3060 Ti
cuda: 12.3
cudnn: 8.9.7
tensorflow: 2.15.0
https://jackfrisht.medium.com/install-nvidia-driver-via-ppa-in-ubuntu-18-04-fc9a8c4658b9


CNN, epochs = 200, lr = 0.0001, batch_size = 128
SNR(dB)		NMSE				        Sum rate(bandwidth = 10)
    -10		0.8539942861956278          12.940970626378812
     -5		0.5995452987007895          13.972075582297652
      0		0.41533541720470285         14.508584726671309
      5		0.28924182532338316         14.825802753405233
     10		0.25099049641568744         14.9719951899972
     15		0.2179765096864287          15.106508174427415
     20		0.1962879938877405          15.093083672359931     
---------------------------------------------------------------------------------------------
Transformers(Encoder, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.030168386176228523            15.578949326993863
     -5          0.020958859473466873            15.66211852964721
      0           0.01619298756122589            15.704970515352377
      5          0.013632509857416153            15.727940458993261
     10          0.012119762599468231            15.741494041206668
     15          0.011332360096275806            15.748543839189978
     20           0.01102001965045929            15.751339345320154
     
Transformers(Encoder, (4 * 16 * 32)), epochs = 200, lr = 0.0001, batch_size = 32
SNR(dB)		NMSE				Sum rate(bandwidth = 10)
    -10            0.0363365039229393            15.523000001405535
     -5          0.025771625339984894            15.618731246613217
      0           0.01936345174908638            15.676489753618485
      5           0.01621336117386818            15.704797622232165
     10           0.01420888677239418             15.72278177831539
     15          0.012929177843034267            15.734251528601643
     20          0.012564736418426037            15.737516305899089

Transformers(Encoder, (32 * 16 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03114357590675354            15.570141174820176
     -5          0.021705850958824158            15.655409034506654
      0           0.01664380356669426            15.700936614010356
      5          0.013890516944229603            15.725639271869218
     10          0.012430176138877869            15.738724399857283
     15          0.011685009114444256            15.745396795766773
     20          0.011496438644826412            15.747084795220099

Transformers(Encoder, originnal shape), epochs = 200, lr = 0.0001, batch_size = 32
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.09679478406906128            15.376328029137811
     -5           0.05998114123940468            15.558169969590026
      0           0.03382720798254013            15.685979304816946
      5           0.03369864076375961             15.68660493095477
     10           0.02343943528831005            15.736430019806559
     15          0.029891125857830048             15.70511663172949
     20          0.022128328680992126            15.742785169835097
---------------------------------------------------------------------------------------------
Transformers(Encoder * 4 + Decoder * 4, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10		0.030041592195630074		15.580097663387434
     -5		0.020925084128975868		15.662422590888331(0.02063744328916073, 15.66501245553348)
      0		0.016060134395956993		15.70616330179968
      5		0.01353185623884201		15.72884259760811
     10		0.01198374293744564		15.74271209404264
     15		0.011237981729209423		15.749388603678916
     20		0.010901781730353832		15.75239740560448

Transformers(Encoder * 4 + Decoder * 4, (32 * 16 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10		0.03053562343120575		15.575649165235863
     -5		0.02151232771575451		15.657152143870377
      0		0.016592485830187798		15.701397482164204
      5		0.013844613917171955		15.7260508584098
     10		0.012450876645743847		15.73853903358405
     15		0.011729640886187553		15.744997238517547
     20		0.01120199915021658		15.749720161849886

Transformers(Encoder * 4 + Decoder * 4, (4 * 16 * 32)), epochs = 200, lr = 0.0001, batch_size = 32
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10		0.038130421191453934		15.506681251466436		
     -5		0.026996061205863953		15.607668816520237
      0		0.026695137843489647		15.610388424190411
      5		0.02114015258848667		15.660499034739804
     10		0.01389247365295887		15.725618547147068
     15		0.012678253464400768		15.736499516476517
     20	        0.01232545729726553		15.739659440526871		

Transformers(Encoder * 4 + Decoder * 4, originnal shape), epochs = 200, lr = 0.0001, batch_size = 32
SNR(dB)		NMSE				Sum rate(bandwidth = 10)
    -10		
     -5		
      0		
      5		
     10		
     15		
     20	        
---------------------------------------------------------------------------------------------
Transformers(Encoder * 3 + Decoder * 3, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, decoder 有 mask
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.031660910695791245             15.56542539475691
     -5          0.023208769038319588            15.641844298837185
      0          0.017391914501786232             15.69420245522329
      5          0.014003201387822628             15.72461722262373
     10           0.01258099265396595             15.73736293004918
     15          0.012034251354634762            15.742259803203385
     20          0.011669430881738663             15.74552633958734

Transformers(Encoder * 3 + Decoder * 3, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, decoder 有 mask, stride_sparse_self_attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03285108879208565              15.5546319101134
     -5            0.0230310820043087            15.643446541205929
      0           0.01766713708639145             15.69172941135746
      5          0.014568011276423931            15.719552365946374
     10          0.013989264145493507            15.724742243510288
     15           0.01138947531580925             15.74803261180948
     20          0.011203959584236145            15.749693078885972

Transformers(Encoder * 3 + Decoder * 3, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, decoder 有 mask, local_self_attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.034291502088308334            15.541558425324208
     -5          0.025741055607795715            15.618991486838459
      0           0.01740306243300438             15.69410226947647
---------------------------------------------------------------------------------------------
Transformers(Encoder * 9 + Decoder * 0, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, decoder 有 mask
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03453879430890083            15.539312784548422
     -5          0.026762498542666435            15.609762987385716
      0          0.021355926990509033            15.658542552063942
      5           0.01854751817882061             15.68381586680459
     10          0.016703907400369644            15.700382733433553
     15          0.015434942208230495            15.711774756083067
     20          0.013476668857038021            15.729337311794527
