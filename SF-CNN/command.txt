command:
python3 SF_CNN_2fre_train.py > train1.log 2>&1
python3 SF_CNN_2fre_train_further.py > train2.log 2>&1
python3 SF_CNN_2fre_test.py > test.log 2>&1

GPU: NVIDIA GeForce RTX 3060 Ti
cuda: 12.3
cudnn: 8.9.7
tensorflow: 2.15.0
https://jackfrisht.medium.com/install-nvidia-driver-via-ppa-in-ubuntu-18-04-fc9a8c4658b9


CNN, epochs = 200, lr = 0.0001, batch_size = 128
SNR(dB)		NMSE				        Sum rate(bandwidth = 10)
    -10		0.8539942861956278          12.940970626378812
     -5		0.5995452987007895          13.972075582297652
      0		0.41533541720470285         14.508584726671309
      5		0.28924182532338316         14.825802753405233
     10		0.25099049641568744         14.9719951899972
     15		0.2179765096864287          15.106508174427415
     20		0.1962879938877405          15.093083672359931
---------------------------------------------------------------------------------------------
Transformers(Encoder * 9 + Decoder * 0, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, decoder 有 mask
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03453879430890083            15.539312784548422
     -5          0.026762498542666435            15.609762987385716
      0          0.021355926990509033            15.658542552063942
      5           0.01854751817882061             15.68381586680459
     10          0.016703907400369644            15.700382733433553
     15          0.015434942208230495            15.711774756083067
     20          0.013476668857038021            15.729337311794527
---------------------------------------------------------------------------------------------
Transformers(Encoder * 3 + Decoder * 3, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, 無 sparse attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.031660910695791245             15.56542539475691
     -5          0.023208769038319588            15.641844298837185
      0          0.017391914501786232             15.69420245522329
      5          0.014003201387822628             15.72461722262373
     10           0.01258099265396595             15.73736293004918
     15          0.012034251354634762            15.742259803203385
     20          0.011669430881738663             15.74552633958734

Transformers(Encoder * 3 + Decoder * 3, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, stride_sparse_self_attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03285108879208565              15.5546319101134
     -5            0.0230310820043087            15.643446541205929
      0           0.01766713708639145             15.69172941135746
      5          0.014568011276423931            15.719552365946374
     10          0.013989264145493507            15.724742243510288
     15           0.01138947531580925             15.74803261180948
     20          0.011203959584236145            15.749693078885972

Transformers(Encoder * 3 + Decoder * 3, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, local_self_attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.034291502088308334            15.541558425324208
     -5          0.025741055607795715            15.618991486838459
      0           0.01740306243300438             15.69410226947647
      5          0.013940487988293171            15.725179520479465
     10          0.012150107882916927            15.741222315527239
     15          0.011353292502462864            15.748356436102494
     20          0.011100418865680695            15.750619762581621

Transformers(Encoder * 3 + Decoder * 3, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03277484327554703             15.55532376144608
     -5          0.022869588807225227            15.644902614717342
      0           0.01983608677983284             15.67222529243178
      5          0.013921788893640041             15.72534715048587
     10          0.012456191703677177            15.738480920042843
     15          0.012495182454586029              15.7381316534288
     20          0.010741359554231167            15.753832913509044
---------------------------------------------------------------------------------------------
Transformers(Encoder * 3 + Decoder * 3, (16 * 32 * 4)), epochs = 400, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, 無 sparse attention, ff layer 用 Conv1D
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03439707309007645            15.540599845944227
     -5          0.026073306798934937            15.615990333026495
      0          0.017007041722536087            15.697660026461994
      5          0.014019380323588848             15.72447224009828
     10           0.012292718514800072            15.739945099336358
     15          0.011388428509235382            15.748042005613945
     20          0.01079531293362379            15.753350134796849

Transformers(Encoder * 3 + Decoder * 3, (16 * 32 * 4)), epochs = 400, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, 無 sparse attention, ff layer 用 Dilated Conv1D
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03442930802702904            15.540306972826388
     -5          0.028593169525265694            15.593208748802368
      0          0.023658987134695053            15.637783855550557
      5          0.014028274454176426            15.724392414035528
     10          0.012238538824021816               15.740430346128
     15           0.01129980944097042              15.7488351734291
     20          0.011165780015289783            15.750034830624042