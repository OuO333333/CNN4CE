CNN, epochs = 200, lr = 0.0001, batch_size = 128
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10            0.6419357114932095            12.376030106733767
     -5            0.3948697806398535            13.813554657995969
      0            0.2653386565881674             14.51362168247246
      5           0.16621672269013613            15.027294189530807
     10           0.12786874896302208            15.221216724747757
     15            0.1266478453073014            15.227348121333408
     20           0.11196449103168699            15.300884747531022
---------------------------------------------------------------------------------------------
Transformers(Encoder * 9, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.039898015558719635            15.490347333089433
     -5          0.039575472474098206            15.493287869347391
      0           0.03917403519153595            15.496947047258363
      5          0.01579899713397026             15.708424724468355
     10          0.015518935397267342            15.710939771297618
     15          0.015378423035144806            15.712201425164704
     20          0.013160046190023422            15.732106112054788
---------------------------------------------------------------------------------------------
new!
Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, 沒有 sparse attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03965744748711586            15.492540660394557
     -5          0.028810562565922737             15.59108972424448
      0           0.02021711692214012            15.668690136608705
      5          0.015879059210419655            15.707705615158078
     10           0.01424616202712059            15.722364224883243
     15          0.013560600578784943             15.72851409896477
     20          0.013357477262616158            15.730335747297762

new!
Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03856229782104492            15.502521089647496
     -5          0.027260668575763702             15.60511641373273
      0          0.020888376981019974            15.662643520026297
      5          0.019930638372898102            15.671269878086171
     10          0.013781104236841202             15.72653634091522
     15          0.013130326755344868            15.732372591531984
     20          0.012763812206685543            15.735658495479676

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, local_self_attention(2)
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03928970918059349            15.495892652680121
     -5          0.028576577082276344            15.593208143779389
      0           0.02001512609422207            15.670509142513932
      5          0.015685435384511948            15.709444554223042
     10          0.013992800377309322            15.724637358010378
     15           0.013519784435629845            15.728880197789383
     20          0.013477599248290062            15.729258483011803

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, stride_sparse_self_attention(2, 2), no(6, 2), (4, 2)because is worse
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.039743758738040924            15.491753832581484
     -5          0.027776280418038368            15.600451582711955
      0          0.019192421808838844            15.677915513269225
      5          0.014876648783683777            15.716706038110257
     10          0.012890099547803402            15.734526410074793
     15          0.012358461506664753            15.739291727207052
     20          0.012501830235123634            15.738006738750139
---------------------------------------------------------------------------------------------
new!
Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention, ff layer 用 Conv1D
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03798374906182289            15.507790908162843
     -5          0.024973848834633827            15.625787491983507
      0          0.020242398604750633            15.668462428433932
      5          0.015106507577002048            15.714642721214037
     10          0.013501455076038837            15.729044567012842
     15          0.012612905353307724             15.73701119848958
     20          0.012577512301504612            15.737328425151173

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention, ff layer 用 Conv1D, 使用 BatchNormalization(epsilon=1e-6)
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.05668972060084343            15.336422811727001
     -5          0.028094256296753883            15.597574072960933
      0          0.020573817193508148            15.665477346013974
      5          0.028254395350813866            15.596124639155295
     10           0.02500385418534279            15.625516490153341
     15          0.020342955365777016            15.667556703768637
     20            0.0244763046503067            15.630281030850368

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention, ff layer 用 Conv1D, change norm position only on encoder
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.039442263543605804            15.494502152019606
     -5           0.02432405948638916            15.631655688035869
      0          0.021599117666482925             15.65623851411047
      5          0.016517218202352524            15.701972776773513
     10          0.015613597817718983            15.710089680065902
     15          0.014050054363906384            15.724123697543275
     20          0.012439227662980556            15.738567858253838


Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention, ff layer 用 Conv1D
encoder 用: attention(fft * 3, time domain * 3) -> feed forward
decoder 用: attention(fft * 3, time domain * 3) -> attention(fft * 3, time domain * 3) -> feed forward
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.035829074680805206            15.527400195914343
     -5           0.02578163519501686            15.618489127350701
      0           0.01877862773835659            15.681639227262288
      5           0.01463963184505701            15.718833417879862
     10          0.013431007042527199            15.729676387753004
     15          0.011892467737197876            15.743467362565859
     20          0.011549001559615135            15.746544264029659

new!
Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention, ff layer 用 Conv1D
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.039798736572265625            15.491252307075063
     -5           0.02785659208893776            15.599724859887745
      0           0.01906939037144184            15.679022747201321
      5           0.01520221121609211            15.713783557618887
     10           0.01320735178887844            15.731681926378599
     15          0.012557437643408775            15.737508376413132
     20           0.01243357453495264            15.738618518659312
---------------------------------------------------------------------------------------------
