CNN, epochs = 200, lr = 0.0001, batch_size = 128
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10            0.6419357114932095            12.376030106733767
     -5            0.3948697806398535            13.813554657995969
      0            0.2653386565881674             14.51362168247246
      5           0.16621672269013613            15.027294189530807
     10           0.12786874896302208            15.221216724747757
     15            0.1266478453073014            15.227348121333408
     20           0.11196449103168699            15.300884747531022
---------------------------------------------------------------------------------------------
Transformers(Encoder * 9, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03946969285607338            15.494252125551228
     -5          0.039575472474098206            15.493287869347391
      0           0.03917403519153595            15.496947047258363
      5          0.039228908717632294             15.49644670650207
     10          0.015518935397267342            15.710939771297618
     15          0.015378423035144806            15.712201425164704
     20          0.013160046190023422            15.732106112054788
---------------------------------------------------------------------------------------------
Transformers(Encoder * 3 + Decoder * 3, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, 沒有 sparse attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.039754822850227356            15.491652906034776
     -5          0.028318120166659355             15.59554787016789
      0          0.019758177921175957            15.672822676747336
      5          0.015139447525143623              15.7143469884123
     10          0.019137108698487282            15.678413368342067
     15          0.012417358346283436            15.738763865963282
     20          0.013834553770720959            15.726056938180783

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, 沒有 sparse attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03957320749759674            15.493308527872895
     -5            0.0313139446079731            15.568404974771003
      0          0.018902089446783066            15.680528308855868
      5          0.015466086566448212            15.711414292671664
     10          0.013048394583165646            15.733107185589379
     15          0.012472232803702354             15.73827208055814
     20           0.01183935534209013             15.74394318113315

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention
# not yet
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.032459087669849396            15.558185465907597
     -5          0.021085403859615326            15.660977481197651
      0          0.015914564952254295            15.707468871895095
      5          0.012831115163862705            15.735121293979006
     10          0.011864903382956982            15.743775339387081
     15          0.011223804205656052            15.749514615639303
     20          0.010580628179013729            15.755270250083662

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, local_self_attention
# not yet
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03189250826835632            15.563323394555795
     -5           0.02233617939054966             15.64970900162957
      0          0.016399750486016273            15.703112870690317
      5          0.013261282816529274            15.731266684382328
     10          0.011701206676661968            15.745241032091705
     15          0.010796995833516121            15.753334309883158
     20          0.010449905879795551            15.756439769949978

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, stride_sparse_self_attention(2, 2), no(6, 2) because is worse, try(4, 2)
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.039743758738040924            15.491753832581484
     -5          0.027776280418038368            15.600451582711955
      0          0.019192421808838844            15.677915513269225
      5          0.014876648783683777            15.716706038110257
     10          0.012890099547803402            15.734526410074793
     15          0.012358461506664753            15.739291727207052
     20          0.012501830235123634            15.738006738750139
---------------------------------------------------------------------------------------------
Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, 沒有 sparse attention, ff layer 用 Conv1D
# not yet
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.027907442301511765            15.599409834565098
     -5           0.01979498192667961            15.672593795206877
      0          0.015026954002678394            15.715434401516926
      5          0.011979128234088421            15.742752593383953
     10           0.01090138591825962            15.752400188216448
     15          0.010780112817883492            15.753485380153638
     20          0.010648981668055058             15.75465866962726

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, stride_sparse_self_attention, ff layer 用 Conv1D
# not yet
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.026461079716682434            15.612484924773147
     -5           0.02001900039613247            15.670577807075269
      0          0.015465594828128815            15.711498480955896
      5          0.012200472876429558            15.740770340016148
     10           0.01161318738013506            15.746029074173723
     15          0.010544214397668839            15.755596040137812
     20          0.010024880059063435            15.760241620702807

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention, ff layer 用 Conv1D
# not yet
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.02655969187617302            15.611593961503145
     -5            0.0202972199767828            15.668073708121833
      0          0.015511349774897099             15.71108789586881
      5          0.012991032563149929             15.73368843536974
     10          0.01172005757689476            15.745072280499127
     15          0.012593231163918972            15.737252361622446
     20           0.01003818679600954            15.760122626409762