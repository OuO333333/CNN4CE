CNN, epochs = 200, lr = 0.0001, batch_size = 128
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10            0.6419357114932095            12.376030106733767
     -5            0.3948697806398535            13.813554657995969
      0            0.2653386565881674             14.51362168247246
      5           0.16621672269013613            15.027294189530807
     10           0.12786874896302208            15.221216724747757
     15            0.1266478453073014            15.227348121333408
     20           0.11196449103168699            15.300884747531022
---------------------------------------------------------------------------------------------
Transformers(Encoder * 9, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.039898015558719635            15.490347333089433
     -5          0.039575472474098206            15.493287869347391
      0           0.03917403519153595            15.496947047258363
      5          0.01579899713397026             15.708424724468355
     10          0.015518935397267342            15.710939771297618
     15          0.015378423035144806            15.712201425164704
     20          0.013160046190023422            15.732106112054788
---------------------------------------------------------------------------------------------
Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, 沒有 sparse attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03987005725502968            15.490602115597438
     -5          0.029254980385303497             15.58706522935487
      0           0.02019014209508896             15.66893307641061
      5          0.018056659027934074            15.688134016492416
     10          0.015157107263803482            15.714188500585596
     15          0.014463509432971478            15.720413966130947
     20          0.013804900459945202            15.726322933376025

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03955085948109627            15.493512271257995
     -5          0.027489781379699707            15.603043863477247
      0          0.020104501396417618            15.669704313731152
      5           0.01623721234500408            15.704488501364377
     10          0.014446022920310497             15.72057086775953
     15            0.0135115347802639            15.728954210281074
     20           0.01348433643579483            15.729198126988027

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, local_self_attention(2)
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.039571523666381836            15.493323801466287
     -5           0.02931174635887146            15.586551055087641
      0          0.020710840821266174            15.664242912388673
      5           0.01748591847717762            15.693266228231288
     10          0.015006369911134243            15.715541668319254
     15          0.014497745782136917            15.720106725025246
     20          0.013845689594745636            15.725957041500711

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, stride_sparse_self_attention(2, 2), no(6, 2), (4, 2)because is worse
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03988085314631462             15.49050379312277
     -5           0.03053235076367855             15.57549128270972
      0          0.021756574511528015            15.654819170262888
      5          0.016308283433318138            15.703850004969617
     10           0.01488190982490778            15.716658933437483
     15          0.014069385826587677             15.72395028205925
     20          0.013697362504899502            15.727287494963191
---------------------------------------------------------------------------------------------
Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention, ff layer 用 Conv1D
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03733016178011894            15.513741995396554 up to 03990931808948517
     -5          0.026352835819125175            15.613326189757453 up to 027489781379699707
      0           0.01988144963979721             15.67171283671444 up to 020104501396417618
      5          0.015685485675930977             15.70944412424403 up to 01623721234500408
     10          0.014073392376303673            15.723914389956803 up to 014446022920310497
     15          0.013301283121109009            15.730839697733403 up to 0135115347802639
     20           0.01246847864240408            15.738305735345495 up to 01348433643579483
---------------------------------------------------------------------------------------------
Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, full attention
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.039829012006521225             15.49097643884792 better than no sparse
     -5           0.02747759222984314            15.603153955212745 better than no sparse
      0          0.019949764013290405            15.671097689190924 better than no sparse
      5          0.017768146470189095            15.690728563181299 better than no sparse
     10          0.014825291931629181            15.717167092546497 better than no sparse
     15          0.013869925402104855            15.725739653022522 better than no sparse
     20           0.01333386730402708            15.730547503306035 better than no sparse

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention,
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03944879397749901            15.494442592010083 better than no sparse
     -5          0.026776893064379692            15.609491910438882 better than no sparse
      0           0.01985848695039749            15.671919573771865 better than atrous
      5          0.015884367749094963            15.707657914006779 better than no sparse
     10          0.013839473016560078            15.726012765282944 better than atrous
     15          0.013212382793426514            15.731636832092104 better than atrous
     20          0.012837452813982964            15.734998353604407 better than atrous

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, Local Self Attention
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03952145203948021            15.493780284204943 0.039571523666381836, better than local
     -5          0.028553225100040436            15.593419636118835 0.02931174635887146, better than local
      0           0.02063240297138691            15.664949597637182 0.020710840821266174, better than local
      5           0.01693073660135269            15.698256769090925 0.01748591847717762, better than local
     10          0.014729450456798077            15.718027282607842 0.015006369911134243, better than local
     15          0.014260709285736084            15.722233706836965 0.014497745782136917, better than local
     20          0.013719967566430569            15.727084762060883 0.013845689594745636, better than local

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, Stride Sparse Self Attention
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03975171595811844            15.491681067760004 0.03988085314631462, better than stride
     -5           0.02894938737154007            15.589832575098683 0.03053235076367855, better than stride
      0          0.020740732550621033            15.663973687062441 0.021756574511528015, better than stride
      5          0.016013476997613907            15.706498242577116 0.016308283433318138, better than stride
     10          0.014851024374365807            15.716936093266758 0.01488190982490778, better than stride
     15          0.013610122725367546            15.728069976733451 0.014069385826587677, better than stride
     20          0.013560788705945015            15.728512427221728 0.013697362504899502, better than stride
---------------------------------------------------------------------------------------------
