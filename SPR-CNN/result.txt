CNN, epochs = 200, lr = 0.0001, batch_size = 128
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10            0.6419357114932095            12.376030106733767
     -5            0.3948697806398535            13.813554657995969
      0            0.2653386565881674             14.51362168247246
      5           0.16621672269013613            15.027294189530807
     10           0.12786874896302208            15.221216724747757
     15            0.1266478453073014            15.227348121333408
     20           0.11196449103168699            15.300884747531022
---------------------------------------------------------------------------------------------
Transformers(Encoder * 9, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.039898015558719635            15.490347333089433
     -5          0.039575472474098206            15.493287869347391
      0           0.03917403519153595            15.496947047258363
      5          0.01579899713397026             15.708424724468355
     10          0.015518935397267342            15.710939771297618
     15          0.015378423035144806            15.712201425164704
     20          0.013160046190023422            15.732106112054788
---------------------------------------------------------------------------------------------
Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, 沒有 sparse attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03957320749759674            15.493308527872895
     -5            0.0313139446079731            15.568404974771003
      0          0.018902089446783066            15.680528308855868
      5          0.015466086566448212            15.711414292671664
     10          0.013048394583165646            15.733107185589379
     15          0.012472232803702354             15.73827208055814
     20           0.01183935534209013             15.74394318113315

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03994922712445259            15.489880421079087
     -5           0.02725178748369217            15.605196758612788
      0          0.018875764682888985            15.680765231318805
      5          0.014742224477231503            15.717912628820924
     10          0.012882505543529987            15.734594474390205
     15          0.011958373710513115            15.742876849423679
     20          0.011849570088088512            15.743851673854186

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, local_self_attention
# not yet
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03189250826835632            15.563323394555795
     -5           0.02233617939054966             15.64970900162957
      0          0.016399750486016273            15.703112870690317
      5          0.013261282816529274            15.731266684382328
     10          0.011701206676661968            15.745241032091705
     15          0.010796995833516121            15.753334309883158
     20          0.010449905879795551            15.756439769949978

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, stride_sparse_self_attention(2, 2), no(6, 2) because is worse, try(4, 2)
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.039743758738040924            15.491753832581484
     -5          0.027776280418038368            15.600451582711955
      0          0.019192421808838844            15.677915513269225
      5          0.014876648783683777            15.716706038110257
     10          0.012890099547803402            15.734526410074793
     15          0.012358461506664753            15.739291727207052
     20          0.012501830235123634            15.738006738750139
---------------------------------------------------------------------------------------------
Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, 沒有 sparse attention, ff layer 用 Conv1D
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03682966157793999            15.518297369038985
     -5           0.02552940882742405            15.620768421426954
      0          0.019712641835212708            15.673232751832602 ?????
      5          0.020051293075084686            15.670183484142797 ?????
     10          0.013255149126052856            15.731253377327931 ?????
     15           0.01305304653942585            15.733065508536521 ?????
     20          0.011799056082963943             15.74430421121031

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, stride_sparse_self_attention, ff layer 用 Conv1D
# not yet
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.026461079716682434            15.612484924773147
     -5           0.02001900039613247            15.670577807075269
      0          0.015465594828128815            15.711498480955896
      5          0.012200472876429558            15.740770340016148
     10           0.01161318738013506            15.746029074173723
     15          0.010544214397668839            15.755596040137812
     20          0.010024880059063435            15.760241620702807

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention, ff layer 用 Conv1D
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03621919825673103            15.523851843084003
     -5            0.0272029098123312            15.605638873813945
      0          0.017789877951145172            15.690533174195762
      5          0.014526165090501308             15.71985169518842
     10          0.012576806358993053             15.73733479108248
     15          0.011595480144023895              15.7461279053344
     20          0.011442258022725582            15.747500409925326