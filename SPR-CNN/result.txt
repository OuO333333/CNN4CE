CNN, epochs = 200, lr = 0.0001, batch_size = 128
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10            0.6419357114932095            12.376030106733767
     -5            0.3948697806398535            13.813554657995969
      0            0.2653386565881674             14.51362168247246
      5           0.17596123561662536             14.97759882683261
     10           0.13396011468502073            15.190586800457142
     15           0.11912793777809844            15.265055865213462
     20           0.11196449103168699            15.300884747531022
---------------------------------------------------------------------------------------------
Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, 沒有 sparse attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03987005725502968            15.490602115597438
     -5          0.029254980385303497             15.58706522935487
      0           0.02159520797431469            15.656273772156585
      5          0.018056659027934074            15.688134016492416
     10          0.016204895451664925             15.70477877832414
     15           0.01474946178495884             15.71784768980056
     20          0.014378390274941921            15.721177762372365
---------------------------------------------------------------------------------------------
Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, full attention
encoder ff layer 用 ffn, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.039773717522621155             15.49148045194048 0.03987005725502968 to 0.039645493030548096
     -5          0.028461197391152382            15.594252673970859 0.029254980385303497 to 0.02747759222984314
      0          0.020952610298991203            15.662064721667459 0.02159520797431469 to 0.019949764013290405
      5          0.017375558614730835            15.694258319307453 0.018056659027934074 to 0.016916004940867424 bigger
     10           0.01585640199482441            15.707909134999468 0.016204895451664925 to 0.014825291931629181
     15          0.014552067965269089            15.719619257954932 0.01474946178495884 to 0.013869925402104855
     20          0.014185205101966858            15.722911156969142 0.014378390274941921 to 0.01333386730402708

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, full attention
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.039645493030548096             15.49264954985823
     -5           0.02747759222984314            15.603153955212745
      0          0.019949764013290405            15.671097689190924
      5          0.016916004940867424            15.698389138207103
     10          0.014825291931629181            15.717167092546497
     15          0.013869925402104855            15.725739653022522
     20           0.01333386730402708            15.730547503306035

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention,
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03944879397749901            15.494442592010083
     -5          0.026776893064379692            15.609491910438882
      0           0.01985848695039749            15.671919573771865
      5          0.015884367749094963            15.707657914006779
     10          0.013839473016560078            15.726012765282944
     15          0.013212382793426514            15.731636832092104
     20          0.012837452813982964            15.734998353604407

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, Local Self Attention
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03952145203948021            15.493780284204943 0.039571523666381836, better than local
     -5          0.028553225100040436            15.593419636118835 0.02931174635887146, better than local
      0           0.02063240297138691            15.664949597637182 0.020710840821266174, better than local
      5           0.01693073660135269            15.698256769090925 0.01748591847717762, better than local
     10          0.014729450456798077            15.718027282607842 0.015006369911134243, better than local
     15          0.014260709285736084            15.722233706836965 0.014497745782136917, better than local
     20          0.013719967566430569            15.727084762060883 0.013845689594745636, better than local

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, Stride Sparse Self Attention
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03975171595811844            15.491681067760004 0.03988085314631462, better than stride
     -5           0.02894938737154007            15.589832575098683 0.03053235076367855, better than stride
      0          0.020740732550621033            15.663973687062441 0.021756574511528015, better than stride
      5          0.016013476997613907            15.706498242577116 0.016308283433318138, better than stride
     10          0.014851024374365807            15.716936093266758 0.01488190982490778, better than stride
     15          0.013610122725367546            15.728069976733451 0.014069385826587677, better than stride
     20          0.013560788705945015            15.728512427221728 0.013697362504899502, better than stride
---------------------------------------------------------------------------------------------
DAECNNATT 200
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10            0.5412381862428629             12.97931951377382
     -5            0.3161114473089512            14.243247478681877
     -5           0.33254079647163465            14.154661877931208
      0            0.2115293457419186            14.794740768464427
      5            0.1564142778360277            15.077112879082799
     10           0.12438449687660279            15.238707802839244
     15           0.10890364229279535            15.316166877397217
     20           0.10238436697803815            15.348662296652947

SPARSEMATTDAE 200
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10            0.5238966308505519            13.080715119036215
     -5           0.32426746977979914            14.199338913358442
      0           0.20119746725457452            14.848096915580719
      5           0.13996944454347623            15.160305529554883
     10           0.11280704789072017            15.296675207596744
     15           0.09475028820637019            15.386621664883695
     20           0.08761300950468075            15.422020653796265

CNN, epochs = 200, lr = 0.0001, batch_size = 128
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10            0.6419357114932095            12.376030106733767
     -5            0.3948697806398535            13.813554657995969
      0            0.2653386565881674             14.51362168247246
      5           0.17596123561662536             14.97759882683261
     10           0.13396011468502073            15.190586800457142
     15           0.11912793777809844            15.265055865213462
     20           0.11196449103168699            15.300884747531022
