CNN, epochs = 200, lr = 0.0001, batch_size = 128
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10            0.6419357114932095            12.376030106733767
     -5            0.3948697806398535            13.813554657995969
      0            0.2653386565881674             14.51362168247246
      5           0.16621672269013613            15.027294189530807
     10           0.12786874896302208            15.221216724747757
     15            0.1266478453073014            15.227348121333408
     20           0.11196449103168699            15.300884747531022
---------------------------------------------------------------------------------------------
Transformers(Encoder * 9, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.039898015558719635            15.490347333089433
     -5          0.039575472474098206            15.493287869347391
      0           0.03917403519153595            15.496947047258363
      5          0.01579899713397026             15.708424724468355
     10          0.015518935397267342            15.710939771297618
     15          0.015378423035144806            15.712201425164704
     20          0.013160046190023422            15.732106112054788
---------------------------------------------------------------------------------------------
Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, 沒有 sparse attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03957320749759674            15.493308527872895
     -5            0.0313139446079731            15.568404974771003
      0          0.018902089446783066            15.680528308855868
      5          0.015466086566448212            15.711414292671664
     10          0.013048394583165646            15.733107185589379
     15          0.012472232803702354             15.73827208055814
     20           0.01183935534209013             15.74394318113315

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03793514147400856            15.50823359352834
     -5           0.02725178748369217            15.605196758612788
      0          0.018875764682888985            15.680765231318805
      5          0.014742224477231503            15.717912628820924
     10          0.012882505543529987            15.734594474390205
     15          0.011958373710513115            15.742876849423679
     20          0.011849570088088512            15.743851673854186

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, local_self_attention(2)
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03928970918059349            15.495892652680121
     -5          0.028576577082276344            15.593208143779389
      0           0.02001512609422207            15.670509142513932
      5          0.015685435384511948            15.709444554223042
     10          0.013992800377309322            15.724637358010378
     15           0.013519784435629845            15.728880197789383
     20          0.013477599248290062            15.729258483011803

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, stride_sparse_self_attention(2, 2), no(6, 2), (4, 2)because is worse
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.039743758738040924            15.491753832581484
     -5          0.027776280418038368            15.600451582711955
      0          0.019192421808838844            15.677915513269225
      5          0.014876648783683777            15.716706038110257
     10          0.012890099547803402            15.734526410074793
     15          0.012358461506664753            15.739291727207052
     20          0.012501830235123634            15.738006738750139
---------------------------------------------------------------------------------------------
Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention, ff layer 用 Conv1D
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03621919825673103            15.523851843084003
     -5            0.0272029098123312            15.605638873813945
      0          0.017789877951145172            15.690533174195762
      5          0.014526165090501308             15.71985169518842
     10          0.012576806358993053             15.73733479108248
     15          0.011850962415337563             15.74383919925955
     20          0.011727375909686089            15.744946353832843

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention, ff layer 用 Conv1D, 使用 BatchNormalization(epsilon=1e-6)
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.05668972060084343            15.336422811727001
     -5          0.028094256296753883            15.597574072960933
      0          0.020573817193508148            15.665477346013974
      5          0.028254395350813866            15.596124639155295
     10           0.02500385418534279            15.625516490153341
     15          0.020342955365777016            15.667556703768637
     20            0.0244763046503067            15.630281030850368

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention, ff layer 用 Conv1D, change norm position only on encoder
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.039442263543605804            15.494502152019606
     -5           0.02432405948638916            15.631655688035869
      0          0.021599117666482925             15.65623851411047
      5          0.016517218202352524            15.701972776773513
     10          0.015613597817718983            15.710089680065902
     15          0.014050054363906384            15.724123697543275
     20          0.012439227662980556            15.738567858253838

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention, ff layer 用 Conv1D
encoder 用: attention -> attention(fft) -> feed forward
decoder 用: attention -> attention -> feed forward
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03557024896144867            15.529753857699406
     -5          0.028089318424463272            15.597618809774684
      0          0.021435804665088654            15.657710510065908
      5          0.014477238059043884            15.720290763950253
     10          0.012947558425366879            15.734011270221915
     15          0.012077320367097855            15.741811069116425
     20          0.011390415020287037            15.747964716661965

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention, ff layer 用 Conv1D
encoder 用: attention(fft) -> feed forward
decoder 用: attention(fft) -> attention(fft) -> feed forward
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03967680409550667             15.49236413764586
     -5          0.025342775508761406            15.622454754746562
      0           0.01927417330443859            15.677179725673255
      5           0.01661061868071556            15.701133559365285
     10          0.012856674380600452            15.734826055396603
     15          0.011756991036236286            15.744681072164774
     20           0.01172405481338501            15.744976126206147

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention, ff layer 用 Conv1D
encoder 用: attention(fft * 3, time domain * 3) -> feed forward
decoder 用: attention(fft * 3, time domain * 3) -> attention(fft * 3, time domain * 3) -> feed forward
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03627924621105194            15.523305256041029
     -5          0.026568803936243057            15.611373565405362
      0            0.0195012204349041            15.675135991367526
      5           0.01463963184505701            15.718833417879862
     10          0.013460736721754074            15.729409751447445(X)
     15          0.011886927299201488            15.743516977435835(X)
     20          0.011751681566238403            15.744728634603103(X)

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention, ff layer 用 Conv1D
encoder 用: attention(fft * 2, time domain * 2) -> feed forward
decoder 用: attention(fft * 2, time domain * 2) -> attention(fft * 2, time domain * 2) -> feed forward
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10            0.0399286113679409            15.490068199085732
     -5          0.028388243168592453            15.594913056532302
      0          0.018766257911920547            15.681750581888423
      5          0.015432924032211304            15.711712079403728
     10          0.013039842247962952            15.733183917369576
     15          0.012250501662492752            15.740259227509197
     20           0.01175505481660366            15.744698404257917
---------------------------------------------------------------------------------------------
