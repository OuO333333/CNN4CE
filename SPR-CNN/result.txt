CNN, epochs = 200, lr = 0.0001, batch_size = 128
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10            0.6419357114932095            12.376030106733767
     -5            0.3948697806398535            13.813554657995969
      0            0.2653386565881674             14.51362168247246
      5           0.17596123561662536             14.97759882683261
     10           0.13396011468502073            15.190586800457142
     15           0.11912793777809844            15.265055865213462
     20           0.11196449103168699            15.300884747531022
---------------------------------------------------------------------------------------------
Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, 沒有 sparse attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03987005725502968            15.490602115597438
     -5          0.029254980385303497             15.58706522935487
      0           0.02159520797431469            15.656273772156585
      5          0.018056659027934074            15.688134016492416
     10          0.016204895451664925             15.70477877832414
     15           0.01474946178495884             15.71784768980056
     20          0.014378390274941921            15.721177762372365
---------------------------------------------------------------------------------------------
Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, full attention, no cross attention
encoder ff layer 用 ffn, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10            0.0398753322660923            15.490554053541645
     -5          0.030808862298727036            15.572984746486938
      0          0.022814031690359116            15.645283507862723
      5          0.018377229571342468            15.685250583857632
     10           0.01656636968255043            15.701531128715258
     15          0.014835546724498272            15.717075024595555
     20           0.01476007979363203            15.717752387349684

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, full attention
encoder ff layer 用 ffn, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.039773717522621155             15.49148045194048
     -5          0.028461197391152382            15.594252673970859
      0          0.020952610298991203            15.662064721667459
      5          0.017375558614730835            15.694258319307453
     10           0.01585640199482441            15.707909134999468
     15          0.014552067965269089            15.719619257954932
     20          0.014185205101966858            15.722911156969142

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, full attention
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.039645493030548096             15.49264954985823
     -5           0.02747759222984314            15.603153955212745
      0          0.019949764013290405            15.671097689190924
      5          0.016916004940867424            15.698389138207103
     10          0.014825291931629181            15.717167092546497
     15          0.013869925402104855            15.725739653022522
     20           0.01333386730402708            15.730547503306035

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention,
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03944879397749901            15.494442592010083
     -5          0.026776893064379692            15.609491910438882
      0           0.01985848695039749            15.671919573771865
      5          0.015884367749094963            15.707657914006779
     10          0.013839473016560078            15.726012765282944
     15          0.013212382793426514            15.731636832092104
     20          0.012837452813982964            15.734998353604407

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, Local Self Attention
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03952145203948021            15.493780284204943 0.039571523666381836, better than local
     -5          0.028553225100040436            15.593419636118835 0.02931174635887146, better than local
      0           0.02063240297138691            15.664949597637182 0.020710840821266174, better than local
      5           0.01693073660135269            15.698256769090925 0.01748591847717762, better than local
     10          0.014729450456798077            15.718027282607842 0.015006369911134243, better than local
     15          0.014260709285736084            15.722233706836965 0.014497745782136917, better than local
     20          0.013719967566430569            15.727084762060883 0.013845689594745636, better than local

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, Stride Sparse Self Attention
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03975171595811844            15.491681067760004 0.03988085314631462
     -5           0.02894938737154007            15.589832575098683 0.03053235076367855
      0          0.020740732550621033            15.663973687062441 0.021756574511528015
      5          0.016013476997613907            15.706498242577116 0.016308283433318138
     10          0.014851024374365807            15.716936093266758 0.01488190982490778
     15          0.013610122725367546            15.728069976733451 0.014069385826587677
     20          0.013560788705945015            15.728512427221728 0.013697362504899502
---------------------------------------------------------------------------------------------
DAECNNATT 200
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10            0.5412381862428629             12.97931951377382
     -5            0.3161114473089512            14.243247478681877
     -5           0.33254079647163465            14.154661877931208
      0            0.2115293457419186            14.794740768464427
      5            0.1564142778360277            15.077112879082799
     10           0.12438449687660279            15.238707802839244
     15           0.10890364229279535            15.316166877397217
     20           0.10238436697803815            15.348662296652947

SPARSEMATTDAE 200
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10            0.5238966308505519            13.080715119036215
     -5           0.32426746977979914            14.199338913358442
      0           0.20119746725457452            14.848096915580719
      5           0.13996944454347623            15.160305529554883
     10           0.11280704789072017            15.296675207596744
     15           0.09475028820637019            15.386621664883695
     20           0.08761300950468075            15.422020653796265

---------------------------------------------------------------------------------------------
FEDformer
Encoder *  2 , Decoder *  2 , reshape_type = (Nr, Nt, channel)
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.04003031924366951            15.489140933602904 0.03987005725502968
     -5          0.032262492924928665             15.55980019276288 0.029254980385303497
      0          0.023152129724621773            15.642233280343834 0.02159520797431469
      5          0.017859801650047302            15.689904380496367 0.018056659027934074 ok
     10           0.01619112305343151             15.70490247599874
     15          0.014731141738593578            15.718012085872813 0.01474946178495884 ok
     20          0.014243846759200096            15.722385011103668 0.014378390274941921 ok
---------------------------------------------------------------------------------------------
Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention, d = 3
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03968484327197075             15.49229083432177
     -5          0.028611933812499046            15.592888048108476
      0          0.023897996172308922            15.635502141705988 
      5           0.02146201953291893             15.65747419928054
     10          0.019011519849300385            15.679543601852913
     15          0.016211938112974167            15.704715510298538
     20           0.01379705686122179            15.726393267161576

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, random_self_attention
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.04192124307155609            15.471887670465838
     -5           0.04125601425766945            15.477959831493383
      0           0.04102673381567001             15.48005189623524
      5            0.0407208614051342            15.482842452373948
     10           0.04050442576408386            15.484816921589978
     15          0.040384791791439056            15.485908037287839
     20            0.0396304652094841             15.49278659806734

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, Local Self Attention, 50%
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.039498768746852875             15.49398709544224 0.03944879397749901-0.03968484327197075
     -5           0.02775953710079193             15.60060311439872 0.026776893064379692-0.028611933812499046
      0          0.020758099853992462            15.663817260607027
      5           0.01766771636903286            15.691631612401814 bigger than 016916004940867424
     10          0.015282352454960346            15.713064030051235
     15          0.014278898946940899             15.72207049582714
     20          0.013560183346271515            15.728517878915074

SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.037190988659858704             15.51500861371505 0.03944879397749901-0.03968484327197075
     -5           0.02775953710079193             15.60060311439872 0.026776893064379692-0.028611933812499046
      0          0.020758099853992462            15.663817260607027
      5            0.0168608445674181            15.698884917423946
     10          0.014770912937819958            15.717655131060475
     15          0.013729386031627655            15.727000243952023   
     20          0.013560183346271515            15.728517878915074