CNN, epochs = 200, lr = 0.0001, batch_size = 128
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10            0.6419357114932095            12.376030106733767
     -5            0.3948697806398535            13.813554657995969
      0            0.2653386565881674             14.51362168247246
      5           0.17596123561662536             14.97759882683261
     10           0.13396011468502073            15.190586800457142
     15           0.11912793777809844            15.265055865213462
     20           0.11196449103168699            15.300884747531022
---------------------------------------------------------------------------------------------
Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, 沒有 sparse attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03987005725502968            15.490602115597438
     -5          0.029254980385303497             15.58706522935487
      0           0.02198883332312107            15.652725337861053
      5          0.018056659027934074            15.688134016492416
     10          0.016204895451664925             15.70477877832414
     15           0.01474946178495884             15.71784768980056
     20          0.014378390274941921            15.721177762372365
---------------------------------------------------------------------------------------------
Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, full attention, no cross attention
encoder ff layer 用 ffn, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10            0.0398753322660923            15.490554053541645
     -5          0.030808862298727036            15.572984746486938
      0          0.022814031690359116            15.645283507862723
      5          0.018377229571342468            15.685250583857632
     10           0.01656636968255043            15.701531128715258
     15          0.014835546724498272            15.717075024595555
     20           0.01476007979363203            15.717752387349684

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, full attention
encoder ff layer 用 ffn, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.039773717522621155             15.49148045194048
     -5          0.028461197391152382            15.594252673970859
      0          0.021421413868665695            15.657840168742075
      5          0.017966583371162415            15.688944079251307
     10           0.01585640199482441            15.707909134999468
     15          0.014552067965269089            15.719619257954932
     20          0.014185205101966858            15.722911156969142

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, Local Self Attention
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.04016707092523575            15.487893916991833
     -5            0.0282458383589983             15.59620204858171
      0          0.020967191085219383            15.661933478931305
      5           0.01693073660135269            15.698256769090925
     10          0.015475446358323097             15.71133025985971
     15          0.014496663585305214            15.720116450547517
     20          0.013719967566430569            15.727084762060883

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, Stride Sparse Self Attention
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03975171595811844            15.491681067760004
     -5           0.02894938737154007            15.589832575098683
     -5          0.027584269642829895            15.602188908767813
      0          0.020740732550621033            15.663973687062441
      5           0.01648659072816372            15.702248021962497
     10          0.014851024374365807            15.716936093266758
     15          0.014254416339099407            15.722290191751824
     20          0.013560788705945015            15.728512427221728
---------------------------------------------------------------------------------------------
DAECNNATT 200
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10            0.5412381862428629             12.97931951377382
     -5            0.3161114473089512            14.243247478681877
     -5           0.33254079647163465            14.154661877931208
      0            0.2115293457419186            14.794740768464427
      5            0.1564142778360277            15.077112879082799
     10           0.12438449687660279            15.238707802839244
     15           0.10890364229279535            15.316166877397217
     20           0.10238436697803815            15.348662296652947

SPARSEMATTDAE 200
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10            0.5238966308505519            13.080715119036215
     -5           0.32426746977979914            14.199338913358442
      0           0.20119746725457452            14.848096915580719
      5           0.13996944454347623            15.160305529554883
     10           0.11280704789072017            15.296675207596744
     15           0.09475028820637019            15.386621664883695
     20           0.08761300950468075            15.422020653796265

---------------------------------------------------------------------------------------------
FEDformer
Encoder *  2 , Decoder *  2 , reshape_type = (Nr, Nt, channel)
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.04003031924366951            15.489140933602904 0.03987005725502968
     -5          0.032262492924928665             15.55980019276288 0.029254980385303497
      0          0.023152129724621773            15.642233280343834 0.02159520797431469
      5          0.017859801650047302            15.689904380496367 0.018056659027934074 ok
     10           0.01619112305343151             15.70490247599874
     15          0.014731141738593578            15.718012085872813 0.01474946178495884 ok
     20          0.014243846759200096            15.722385011103668 0.014378390274941921 ok
---------------------------------------------------------------------------------------------
Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention, d = 3
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03968484327197075             15.49229083432177
     -5          0.028611933812499046            15.592888048108476
      0          0.023897996172308922            15.635502141705988 
      5           0.02146201953291893             15.65747419928054
     10          0.019011519849300385            15.679543601852913
     15          0.016211938112974167            15.704715510298538
     20           0.01379705686122179            15.726393267161576

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, random_self_attention
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.04192124307155609            15.471887670465838
     -5           0.04125601425766945            15.477959831493383
      0           0.04102673381567001             15.48005189623524
      5            0.0407208614051342            15.482842452373948
     10           0.04050442576408386            15.484816921589978
     15          0.040384791791439056            15.485908037287839
     20            0.0396304652094841             15.49278659806734

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, Local Self Attention, 50%
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.039498768746852875             15.49398709544224 0.03944879397749901-0.03968484327197075
     -5           0.02775953710079193             15.60060311439872 0.026776893064379692-0.028611933812499046
      0          0.020758099853992462            15.663817260607027
      5           0.01766771636903286            15.691631612401814 bigger than 016916004940867424
     10          0.015282352454960346            15.713064030051235
     15          0.014278898946940899             15.72207049582714
     20          0.013560183346271515            15.728517878915074
---------------------------------------------------------------------------------------------
Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, full attention
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.039645493030548096             15.49264954985823 lower than 0.039773717522621155
     -5           0.02848387509584427            15.594047444695512 lower than 0.028461197391152382
      0           0.02129441872239113            15.658984741992914 lower than 0.021421413868665695
      5          0.017665809020400047            15.691648785427166 lower than 0.017966583371162415
     10          0.015258733183145523            15.713276056208183 lower than 0.01585640199482441
     15           0.01451767235994339            15.719927910290641 lower than 0.014552067965269089
     20          0.013874311000108719            15.725700256494086 lower than 0.014185205101966858


Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention,
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.038145072758197784            15.506321704157296 lower than 0.039645493030548096
     -5          0.027047526091337204             15.60704432309743 lower than 0.02848387509584427
      0           0.02087853103876114            15.662732238044741 lower than 0.02129441872239113, higher
      5          0.016779784113168716              15.6996133959397 lower than 0.017665809020400047
     10          0.014481263235211372            15.720254642621406 lower than 0.015258733183145523
     15           0.01399294938892126            15.724635949414886 lower than 0.01451767235994339
     20           0.01356890331953764            15.728439669153394 lower than 0.013874311000108719

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_and_local_self_attention, d = 2, windows = 6
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03719576820731163            15.514965219088664 ok
     -5           0.02675441838800907            15.609695093405547 ok
      0           0.02032649889588356            15.667704968574576
      5          0.016275744885206223            15.704142340486547
     10          0.014112374745309353            15.723564602694648 ok
     15           0.01363054383546114            15.727886811139943 ok

     20          0.013208819553256035              15.7316688188335 ok

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_and_local_self_attention, d = 2, windows = 15
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10            0.0398053303360939            15.491192331802184
     -5           0.02896812930703163            15.589662853816055
      0          0.020983632653951645            15.661785332828732
      5          0.021863700821995735            15.653853396908078
     10          0.014343110844492912             15.72149429923586
     15           0.01406656950712204            15.723975574817338

