CNN, epochs = 200, lr = 0.0001, batch_size = 128
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10            0.6419357114932095            12.376030106733767
     -5            0.3948697806398535            13.813554657995969
      0            0.2653386565881674             14.51362168247246
      5           0.16621672269013613            15.027294189530807
     10           0.12786874896302208            15.221216724747757
     15            0.1266478453073014            15.227348121333408
     20           0.11196449103168699            15.300884747531022
---------------------------------------------------------------------------------------------
Transformers(Encoder * 9, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.039898015558719635            15.490347333089433
     -5          0.039575472474098206            15.493287869347391
      0           0.03917403519153595            15.496947047258363
      5          0.01579899713397026             15.708424724468355
     10          0.015518935397267342            15.710939771297618
     15          0.015378423035144806            15.712201425164704
     20          0.013160046190023422            15.732106112054788
---------------------------------------------------------------------------------------------
new!
new!
Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, 沒有 sparse attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03987005725502968            15.490602115597438
     -5          0.029254980385303497             15.58706522935487
      0           0.02019014209508896             15.66893307641061
      5          0.018056659027934074            15.688134016492416
     10          0.015157107263803482            15.714188500585596
     15          0.014463509432971478            15.720413966130947
     20          0.013804900459945202            15.726322933376025

new!
new!
Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03990931808948517            15.490244286888137
     -5          0.027489781379699707            15.603043863477247
      0          0.020104501396417618            15.669704313731152
      5           0.01623721234500408            15.704488501364377
     10          0.014446022920310497             15.72057086775953
     15            0.0135115347802639            15.728954210281074
     20           0.01348433643579483            15.729198126988027

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, local_self_attention(2)
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03928970918059349            15.495892652680121
     -5          0.028576577082276344            15.593208143779389
      0           0.02001512609422207            15.670509142513932
      5          0.015685435384511948            15.709444554223042
     10          0.013992800377309322            15.724637358010378
     15           0.013519784435629845            15.728880197789383
     20          0.013477599248290062            15.729258483011803

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, stride_sparse_self_attention(2, 2), no(6, 2), (4, 2)because is worse
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.039743758738040924            15.491753832581484
     -5          0.027776280418038368            15.600451582711955
      0          0.019192421808838844            15.677915513269225
      5          0.014876648783683777            15.716706038110257
     10          0.012890099547803402            15.734526410074793
     15          0.012358461506664753            15.739291727207052
     20          0.012501830235123634            15.738006738750139
---------------------------------------------------------------------------------------------
new!
Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention, ff layer 用 Conv1D
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03798374906182289            15.507790908162843 up to 03856229782104492
     -5          0.024973848834633827            15.625787491983507 up to 027260668575763702
      0          0.020242398604750633            15.668462428433932 up to 020888376981019974
      5          0.015106507577002048            15.714642721214037 up to 019930638372898102?????
     10          0.013501455076038837            15.729044567012842 up to 013781104236841202
     15          0.012612905353307724             15.73701119848958 up to 013130326755344868
     20          0.012577512301504612            15.737328425151173 up to 012763812206685543

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention, ff layer 用 Conv1D, 使用 BatchNormalization(epsilon=1e-6)
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.05668972060084343            15.336422811727001
     -5          0.028094256296753883            15.597574072960933
      0          0.020573817193508148            15.665477346013974
      5          0.028254395350813866            15.596124639155295
     10           0.02500385418534279            15.625516490153341
     15          0.020342955365777016            15.667556703768637
     20            0.0244763046503067            15.630281030850368

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention, ff layer 用 Conv1D, change norm position only on encoder
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.039442263543605804            15.494502152019606
     -5           0.02432405948638916            15.631655688035869
      0          0.021599117666482925             15.65623851411047
      5          0.016517218202352524            15.701972776773513
     10          0.015613597817718983            15.710089680065902
     15          0.014050054363906384            15.724123697543275
     20          0.012439227662980556            15.738567858253838

new!
Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention, ff layer 用 Conv1D
encoder 用: attention(fft * 3, time domain * 3) -> feed forward
decoder 用: attention(fft * 3, time domain * 3) -> attention(fft * 3, time domain * 3) -> feed forward
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03958628699183464            15.493189470454807
     -5          0.026761602610349655             15.60963012139662
      0           0.01949368789792061            15.675203902295271
      5          0.015964316204190254            15.706939919141396
     10          0.013872623443603516              15.7257154174736
     15          0.013284081593155861            15.730993950016433
     20          0.012256275862455368            15.740207486706483

new!
Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention, ff layer 用 Conv1D
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.039798736572265625            15.491252307075063
     -5           0.02785659208893776            15.599724859887745
      0           0.01906939037144184            15.679022747201321
      5           0.01520221121609211            15.713783557618887
     10           0.01320735178887844            15.731681926378599
     15          0.012557437643408775            15.737508376413132
     20           0.01243357453495264            15.738618518659312
---------------------------------------------------------------------------------------------
