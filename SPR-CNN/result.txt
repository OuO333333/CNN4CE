CNN, epochs = 200, lr = 0.0001, batch_size = 128
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10            0.6419357114932095            12.376030106733767
     -5            0.3948697806398535            13.813554657995969
      0            0.2653386565881674             14.51362168247246
      5           0.16621672269013613            15.027294189530807
     10           0.12786874896302208            15.221216724747757
     15            0.1266478453073014            15.227348121333408
     20           0.11196449103168699            15.300884747531022
---------------------------------------------------------------------------------------------
Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, 沒有 sparse attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03987005725502968            15.490602115597438
     -5          0.029254980385303497             15.58706522935487
      0           0.02159520797431469            15.656273772156585
      5          0.018056659027934074            15.688134016492416
     10          0.016204895451664925             15.70477877832414
     15           0.01474946178495884             15.71784768980056
     20          0.014378390274941921            15.721177762372365
---------------------------------------------------------------------------------------------
Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, full attention
encoder ff layer 用 ffn, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.039773717522621155             15.49148045194048 0.03987005725502968 to 0.039645493030548096
     -5          0.028461197391152382            15.594252673970859 0.029254980385303497 to 0.02747759222984314
      0          0.020952610298991203            15.662064721667459 0.02159520797431469 to 0.019949764013290405
      5          0.017375558614730835            15.694258319307453 0.018056659027934074 to 0.016916004940867424 bigger
     10           0.01585640199482441            15.707909134999468 0.016204895451664925 to 0.014825291931629181
     15          0.014552067965269089            15.719619257954932 0.01474946178495884 to 0.013869925402104855
     20          0.014185205101966858            15.722911156969142 0.014378390274941921 to 0.01333386730402708

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, full attention
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.039645493030548096             15.49264954985823
     -5           0.02747759222984314            15.603153955212745
      0          0.019949764013290405            15.671097689190924
      5          0.016916004940867424            15.698389138207103
     10          0.014825291931629181            15.717167092546497
     15          0.013869925402104855            15.725739653022522
     20           0.01333386730402708            15.730547503306035

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention,
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03944879397749901            15.494442592010083
     -5          0.026776893064379692            15.609491910438882
      0           0.01985848695039749            15.671919573771865
      5          0.015884367749094963            15.707657914006779
     10          0.013839473016560078            15.726012765282944
     15          0.013212382793426514            15.731636832092104
     20          0.012837452813982964            15.734998353604407

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, Local Self Attention
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03952145203948021            15.493780284204943 0.039571523666381836, better than local
     -5          0.028553225100040436            15.593419636118835 0.02931174635887146, better than local
      0           0.02063240297138691            15.664949597637182 0.020710840821266174, better than local
      5           0.01693073660135269            15.698256769090925 0.01748591847717762, better than local
     10          0.014729450456798077            15.718027282607842 0.015006369911134243, better than local
     15          0.014260709285736084            15.722233706836965 0.014497745782136917, better than local
     20          0.013719967566430569            15.727084762060883 0.013845689594745636, better than local

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, Stride Sparse Self Attention
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03975171595811844            15.491681067760004 0.03988085314631462, better than stride
     -5           0.02894938737154007            15.589832575098683 0.03053235076367855, better than stride
      0          0.020740732550621033            15.663973687062441 0.021756574511528015, better than stride
      5          0.016013476997613907            15.706498242577116 0.016308283433318138, better than stride
     10          0.014851024374365807            15.716936093266758 0.01488190982490778, better than stride
     15          0.013610122725367546            15.728069976733451 0.014069385826587677, better than stride
     20          0.013560788705945015            15.728512427221728 0.013697362504899502, better than stride
---------------------------------------------------------------------------------------------
DAECNNATT 1000
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03933100774884224             15.49551627493901
     -5           0.03283994644880295            15.554559310004041
      0           0.02780035510659218            15.600233745853647
      5          0.025252509862184525            15.623270281730456
     10          0.020947113633155823            15.662114278080406
     15           0.02840043418109417            15.594802713929688
     20          0.016425205394625664            15.702799520775624

SPARSEMATTDAE 1000
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.039441920816898346            15.494505332564072 x
     -5           0.03918106108903885            15.496882844470836 x
      0          0.030208833515644073            15.578423480537776 x
      5          0.027832571417093277             15.59994218927777 x
     10          0.026343869045376778            15.613407083751447 x
     15          0.025749802589416504            15.618776832036172
     20          0.025760764256119728            15.618677802577267
