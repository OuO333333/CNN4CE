CNN, epochs = 200, lr = 0.0001, batch_size = 128
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10            0.6419357114932095            12.376030106733767
     -5            0.3948697806398535            13.813554657995969
      0            0.2653386565881674             14.51362168247246
      5           0.17596123561662536             14.97759882683261
     10           0.13396011468502073            15.190586800457142
     15           0.11912793777809844            15.265055865213462
     20           0.11196449103168699            15.300884747531022
---------------------------------------------------------------------------------------------
Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, 沒有 sparse attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.040231116116046906            15.487309683281385
     -5          0.030061252415180206            15.579760882190199
      0           0.02286352589726448             15.64483701235907
      5          0.018579063937067986             15.68343480874185
     10          0.016204895451664925             15.70477877832414
     15          0.015272018499672413            15.713156822752882
     20          0.014378390274941921            15.721177762372365
---------------------------------------------------------------------------------------------
Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, full attention, no cross attention
encoder ff layer 用 ffn, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.040216702967882156            15.487441121434859
     -5          0.030808862298727036            15.572984746486938
      0           0.02333066239953041              15.6406224579517
      5          0.019309449940919876            15.676862212688121
     10           0.01656636968255043            15.701531128715258
     15          0.015636218711733818            15.709886586782869
     20           0.01476007979363203            15.717752387349684

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, full attention
encoder ff layer 用 ffn, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.04003896936774254             15.48906192608235
     -5           0.02920401655137539            15.587526682731204
      0          0.021999454125761986            15.652629488346413
      5          0.017966583371162415            15.688944079251307
     10           0.01563282497227192            15.709916997359823
     15           0.01466186810284853            15.718633820350675
     20          0.014185205101966858            15.722911156969142

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, Local Self Attention
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.04016707092523575            15.487893916991833
     -5            0.0282458383589983             15.59620204858171
      0          0.020967191085219383            15.661933478931305
      5           0.01693073660135269            15.698256769090925
     10          0.015475446358323097             15.71133025985971
     15          0.014496663585305214            15.720116450547517
     20          0.013719967566430569            15.727084762060883

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, Stride Sparse Self Attention
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03975171595811844            15.491681067760004
     -5           0.02894938737154007            15.589832575098683
     -5          0.027584269642829895            15.602188908767813
      0          0.020740732550621033            15.663973687062441
      5           0.01648659072816372            15.702248021962497
     10          0.014851024374365807            15.716936093266758
     15          0.014254416339099407            15.722290191751824
     20          0.013560788705945015            15.728512427221728
---------------------------------------------------------------------------------------------
DAECNNATT 200
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10            0.5412381862428629             12.97931951377382
     -5            0.3161114473089512            14.243247478681877
     -5           0.33254079647163465            14.154661877931208
      0            0.2115293457419186            14.794740768464427
      5            0.1564142778360277            15.077112879082799
     10           0.12438449687660279            15.238707802839244
     15           0.10890364229279535            15.316166877397217
     20           0.10238436697803815            15.348662296652947

SPARSEMATTDAE 200
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10            0.5238966308505519            13.080715119036215
     -5           0.32426746977979914            14.199338913358442
      0           0.20119746725457452            14.848096915580719
      5           0.13996944454347623            15.160305529554883
     10           0.11280704789072017            15.296675207596744
     15           0.09475028820637019            15.386621664883695
     20           0.08761300950468075            15.422020653796265

---------------------------------------------------------------------------------------------
FEDformer
Encoder *  2 , Decoder *  2 , reshape_type = (Nr, Nt, channel)
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.04003031924366951            15.489140933602904 0.03987005725502968
     -5          0.032262492924928665             15.55980019276288 0.029254980385303497
      0          0.023152129724621773            15.642233280343834 0.02159520797431469
      5          0.017859801650047302            15.689904380496367 0.018056659027934074 ok
     10           0.01619112305343151             15.70490247599874
     15          0.014731141738593578            15.718012085872813 0.01474946178495884 ok
     20          0.014243846759200096            15.722385011103668 0.014378390274941921 ok
---------------------------------------------------------------------------------------------
Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention, d = 3
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03968484327197075             15.49229083432177
     -5          0.028611933812499046            15.592888048108476
      0          0.023897996172308922            15.635502141705988 
      5           0.02146201953291893             15.65747419928054
     10          0.019011519849300385            15.679543601852913
     15          0.016211938112974167            15.704715510298538
     20           0.01379705686122179            15.726393267161576

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, random_self_attention
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.04192124307155609            15.471887670465838
     -5           0.04125601425766945            15.477959831493383
      0           0.04102673381567001             15.48005189623524
      5            0.0407208614051342            15.482842452373948
     10           0.04050442576408386            15.484816921589978
     15          0.040384791791439056            15.485908037287839
     20            0.0396304652094841             15.49278659806734

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, Local Self Attention, 50%
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.039498768746852875             15.49398709544224
     -5           0.02775953710079193             15.60060311439872
      0          0.020758099853992462            15.663817260607027
      5           0.01766771636903286            15.691631612401814
     10          0.015282352454960346            15.713064030051235
     15          0.014278898946940899             15.72207049582714
     20          0.013560183346271515            15.728517878915074
---------------------------------------------------------------------------------------------
Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, full attention
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.039645493030548096             15.49264954985823 lower than 0.039773717522621155
     -5           0.02848387509584427            15.594047444695512 lower than 0.028461197391152382
      0           0.02177749201655388             15.65463062254004 lower than 0.021421413868665695
      5          0.017665809020400047            15.691648785427166 lower than 0.017966583371162415
     10          0.015258733183145523            15.713276056208183 lower than 0.01585640199482441
     15           0.01451767235994339            15.719927910290641 lower than 0.014552067965269089
     20          0.013874311000108719            15.725700256494086 lower than 0.014185205101966858


Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention,
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.038145072758197784            15.506321704157296
     -5          0.027047526091337204             15.60704432309743 lower than 0.02848387509584427, bigger
      0           0.02087853103876114            15.662732238044741
      5          0.016779784113168716              15.6996133959397
     10          0.014481263235211372            15.720254642621406
     15           0.01399294938892126            15.724635949414886
     20           0.01356890331953764            15.728439669153394


Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_and_distance_self_attention, d = 2, 0-6, 9-10, 13-14
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.037573281675577164            15.511528572054784
     -5          0.026736173778772354            15.609860095464892
      0           0.02016441710293293            15.669164710042915
      5          0.016315100714564323            15.703788696782645
     10          0.014203720726072788            15.722745045439746
     15          0.013647235929965973            15.727737112861892
     20          0.013198921456933022            15.731757566231902

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, Distance-based Self Attention, 0-6, 9-10, 13-14
encoder ff layer 用 Conv1D, decoder ff layer 用 ffn
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.038724351674318314            15.501044656005213
     -5          0.027728276327252388            15.600885983690185
      0           0.02133762463927269            15.658595399333276
      5          0.017316358163952827            15.694790560164428
     10          0.014847148209810257            15.716970865372833
     15          0.014284402132034302            15.722021087263872
     20          0.013717515394091606            15.727106743935753

---------------------------------------------------------------------------------------------
原始 Transformers
element = 8
     20           0.04024963080883026             15.44227994523937
element = 16
     20          0.016927558928728104            15.676762953712029
element = 32
     20          0.014378390274941921            15.721177762372365
element = 64
     20          0.019694823771715164            15.692043702222328
element = 256
     20          0.028786765411496162            15.643368221732587
---------------------------------------------------------------------------------------------
inference time
CNN:
执行时间： 0.08526277542114258 秒

Attention-aided Auto-Encoder:
执行时间： 0.0691976547241211 秒

Sparse Auto-Encoder:
执行时间： 0.0778207778930664 秒

Transformers:
执行时间： 0.11922812461853027 秒

FEDformer:
执行时间： 0.3617250919342041 秒

Proposed Transformer:
执行时间： 0.16572022438049316 秒



