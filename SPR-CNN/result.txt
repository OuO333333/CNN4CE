CNN, epochs = 200, lr = 0.0001, batch_size = 128
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10            0.6419357114932095            12.376030106733767
     -5            0.3948697806398535            13.813554657995969
      0            0.2653386565881674             14.51362168247246
      5           0.16621672269013613            15.027294189530807
     10           0.12786874896302208            15.221216724747757
     15            0.1266478453073014            15.227348121333408
     20           0.11196449103168699            15.300884747531022
---------------------------------------------------------------------------------------------
Transformers(Encoder * 9, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.039898015558719635            15.490347333089433
     -5          0.039575472474098206            15.493287869347391
      0           0.03917403519153595            15.496947047258363
      5          0.01579899713397026             15.708424724468355
     10          0.015518935397267342            15.710939771297618
     15          0.015378423035144806            15.712201425164704
     20          0.013160046190023422            15.732106112054788
---------------------------------------------------------------------------------------------
Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, 沒有 sparse attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03987005725502968            15.490602115597438
     -5          0.029254980385303497             15.58706522935487
      0           0.02019014209508896             15.66893307641061
      5          0.018056659027934074            15.688134016492416
     10          0.015157107263803482            15.714188500585596
     15          0.014463509432971478            15.720413966130947
     20          0.013804900459945202            15.726322933376025

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03955085948109627            15.493512271257995
     -5          0.027489781379699707            15.603043863477247
      0          0.020104501396417618            15.669704313731152
      5           0.01623721234500408            15.704488501364377
     10          0.014446022920310497             15.72057086775953
     15            0.0135115347802639            15.728954210281074
     20           0.01348433643579483            15.729198126988027

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, local_self_attention(2)
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.039571523666381836            15.493323801466287
     -5           0.02931174635887146            15.586551055087641
      0          0.020710840821266174            15.664242912388673
      5           0.01748591847717762            15.693266228231288
     10          0.015006369911134243            15.715541668319254
     15          0.014497745782136917            15.720106725025246
     20          0.013845689594745636            15.725957041500711

Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, stride_sparse_self_attention(2, 2), no(6, 2), (4, 2)because is worse
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03988085314631462             15.49050379312277
     -5           0.03053235076367855             15.57549128270972
      0          0.021756574511528015            15.654819170262888
      5          0.016308283433318138            15.703850004969617
     10           0.01488190982490778            15.716658933437483
     15          0.014069385826587677             15.72395028205925
     20          0.013697362504899502            15.727287494963191
---------------------------------------------------------------------------------------------
Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention, ff layer 用 Conv1D
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03733016178011894            15.513741995396554 up to 03990931808948517
     -5          0.026352835819125175            15.613326189757453 up to 027489781379699707
      0           0.01988144963979721             15.67171283671444 up to 020104501396417618
      5          0.015685485675930977             15.70944412424403 up to 01623721234500408
     10          0.014073392376303673            15.723914389956803 up to 014446022920310497
     15          0.013301283121109009            15.730839697733403 up to 0135115347802639
     20           0.01246847864240408            15.738305735345495 up to 01348433643579483

# Not yet!
Transformers(Encoder * 2 + Decoder * 2, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention, ff layer 用 Conv1D
modality-aware transformers
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.039798736572265625            15.491252307075063
     -5           0.02785659208893776            15.599724859887745
      0           0.01906939037144184            15.679022747201321
      5           0.01520221121609211            15.713783557618887
     10           0.01320735178887844            15.731681926378599
     15          0.012557437643408775            15.737508376413132
     20           0.01243357453495264            15.738618518659312
---------------------------------------------------------------------------------------------
