CNN, epochs = 200, lr = 0.0001, batch_size = 128
SNR(dB)		NMSE				        Sum rate(bandwidth = 10)
-10         0.649711661089483           14.058652952281365
-5          0.3475041160976275          14.863718856051712
0           0.22225152482196797         15.277555758699604
5           0.14494572869140035         15.44859638003606
10          0.12128514659706721         15.347140577498028
15          0.10142940629835279         15.615965469824227
20          0.08489599243689268         15.62448257698082
---------------------------------------------------------------------------------------------
Transformers(Encoder * 9, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03319474682211876            15.551511616320626
     -5          0.030052466318011284            15.57999690907113
      0          0.016912512481212616            15.698507919501896
      5          0.013853426091372967            15.725959078579967
     10          0.013164506293833256             15.73213395024454
     15           0.011629097163677216            15.745886624325953
     20          0.011142524890601635            15.750242102848484
---------------------------------------------------------------------------------------------
Transformers(Encoder * 3 + Decoder * 3, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, 沒有 sparse attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03295202925801277            15.553713778468975
     -5          0.024725012481212616             15.62816334863857
      0           0.016572220250964165             15.701564150189729
      5          0.01351124607026577            15.72902636704422
     10          0.011882485821843147            15.743617962580778
     15             0.011450755409896374            15.747483214712926
     20          0.010888315737247467            15.752517105400617

Transformers(Encoder * 3 + Decoder * 3, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.032459087669849396            15.558185465907597
     -5          0.021085403859615326            15.660977481197651
      0          0.015914564952254295            15.707468871895095
      5          0.012831115163862705            15.735121293979006
     10          0.011864903382956982            15.743775339387081
     15          0.011223804205656052            15.749514615639303
     20          0.010580628179013729            15.755270250083662

Transformers(Encoder * 3 + Decoder * 3, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, local_self_attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03189250826835632            15.563323394555795
     -5           0.02233617939054966             15.64970900162957
      0          0.016399750486016273            15.703112870690317
      5          0.013261282816529274            15.731266684382328
     10          0.011701206676661968            15.745241032091705
     15          0.010796995833516121            15.753334309883158
     20          0.010449905879795551            15.756439769949978

Transformers(Encoder * 3 + Decoder * 3, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, stride_sparse_self_attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.032578837126493454            15.557099334636352
     -5          0.021571574732661247            15.656598549726002
      0          0.015432968735694885            15.711791330535348
      5          0.012876344844698906            15.734716119263615
     10          0.011555371806025505            15.746546646253226
     15          0.010694130323827267            15.754254715761103
     20          0.010519152507185936            15.755820227182856
---------------------------------------------------------------------------------------------
Transformers(Encoder * 3 + Decoder * 3, (16 * 32 * 4)), epochs = 400, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, 沒有 sparse attention, ff layer 用 Conv1D
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.027907442301511765            15.599409834565098
     -5           0.01979498192667961            15.672593795206877
      0          0.015026954002678394            15.715434401516926
      5          0.011979128234088421            15.742752593383953
     10           0.01090138591825962            15.752400188216448
     15          0.010780112817883492            15.753485380153638
     20          0.010648981668055058             15.75465866962726

Transformers(Encoder * 3 + Decoder * 3, (16 * 32 * 4)), epochs = 400, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, stride_sparse_self_attention, ff layer 用 Conv1D
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.026461079716682434            15.612484924773147
     -5           0.02001900039613247            15.670577807075269
      0          0.015465594828128815            15.711498480955896
      5          0.012200472876429558            15.740770340016148
     10           0.01161318738013506            15.746029074173723
     15          0.010544214397668839            15.755596040137812
     20          0.010024880059063435            15.760241620702807

Transformers(Encoder * 3 + Decoder * 3, (16 * 32 * 4)), epochs = 400, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention, ff layer 用 Conv1D
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.02655969187617302            15.611593961503145
     -5            0.0202972199767828            15.668073708121833
      0          0.015511349774897099             15.71108789586881
      5          0.012991032563149929             15.73368843536974
     10          0.01172005757689476            15.745072280499127
     15          0.011052045039832592            15.751051864640665
     20           0.01003818679600954            15.760122626409762