CNN, epochs = 200, lr = 0.0001, batch_size = 128
SNR(dB)		NMSE				        Sum rate(bandwidth = 10)
5           0.14494572869140035         15.44859638003606
-10         0.649711661089483           14.058652952281365
-5          0.3475041160976275          14.863718856051712
0           0.22225152482196797         15.277555758699604
10          0.12128514659706721         15.347140577498028
15          0.10142940629835279         15.615965469824227
20          0.08489599243689268         15.62448257698082
---------------------------------------------------------------------------------------------
Transformers(Encoder * 3 + Decoder * 3, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, 沒有 sparse attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03295202925801277            15.553713778468975
     -5          0.024725012481212616             15.62816334863857
      0           0.01693081669509411             15.69834350269443
      5          0.014970983378589153            15.715936510814686
     10          0.011882485821843147            15.743617962580778
     15             0.011450755409896374            15.747483214712926
     20          0.010888315737247467            15.752517105400617

Transformers(Encoder * 3 + Decoder * 3, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, atrous_self_attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.032459087669849396            15.558185465907597
     -5          0.021085403859615326            15.660977481197651
      0          0.015914564952254295            15.707468871895095
      5          0.012831115163862705            15.735121293979006
     10          0.011864903382956982            15.743775339387081
     15          0.011223804205656052            15.749514615639303
     20          0.010580628179013729            15.755270250083662

Transformers(Encoder * 3 + Decoder * 3, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, local_self_attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10           0.03189250826835632            15.563323394555795
     -5           0.02233617939054966             15.64970900162957
      0          0.016399750486016273            15.703112870690317
      5          0.013261282816529274            15.731266684382328
     10          0.011701206676661968            15.745241032091705
     15          0.010796995833516121            15.753334309883158
     20          0.010449905879795551            15.756439769949978

Transformers(Encoder * 3 + Decoder * 3, (16 * 32 * 4)), epochs = 200, lr = 0.0001, batch_size = 32
自己寫的 multi-head self attention, stride_sparse_self_attention
SNR(dB)                          NMSE      Sum rate(bandwidth = 10)
    -10          0.032578837126493454            15.557099334636352
     -5          0.021571574732661247            15.656598549726002
      0          0.015432968735694885            15.711791330535348
      5          0.012876344844698906            15.734716119263615
     10          0.011555371806025505            15.746546646253226
     15          0.010694130323827267            15.754254715761103
     20          0.010519152507185936            15.755820227182856